probit.out$par
 as.matrix(solve(probit.out$hessian))
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# Optimize it#
stv <- c(0,0)#
probit.out <-optim(stv,llik.probit,x=cbind(x1),y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))
#
llprobit <- function(b,x,y) {#
  y <- as.matrix(y)#
  x <- as.matrix(x)#
  onevec <- rep(1,nrow(x)) #
  x1 <- cbind(onevec,x)#
  xb <- x1 %*% b #
  return(-sum (y * (log(pnorm(xb))) + (1-y) * log(1-pnorm(xb)))) #
}#
#
xint <-  cbind(rep(1,400),x1)#
bols <- (solve(t(xint) %*% xint)) %*% (t(xint) %*% y)#
#
# estimate the probit model #
probit.out <- optim(bols,llprobit,x=x1,y=y,hessian = T)
probit.out
mod.probit <- glm(formula=y ~ x1,family=binomial(link="probit"))#
#
#Simulating betas#
#library(arm)#
beta <- probit.out$par#
vcmat <- solve(probit.out$hessian)#
#
tab <- array(NA, c(2,2))#
tab[,1] <- beta#
tab[,2] <- c(sqrt(vcmat[1,1]), sqrt(vcmat[2,2]))#
#
n.sim <- 1000#
beta.sim <- mvrnorm(n.sim, beta, vcmat)
S <- 1000										# number of simulations#
sx <- 20#
xx1 <- seq(summary(x1)[1],summary(x1)[6],length.out=sx)  # the variable that will be on the x-axis#
const <- rep(1,sx)								# constant#
#
#Estimated beta and var(beta)#
beta <- probit.out$par#
vcv <- as.matrix(solve(probit.out$hessian))#
#
# Taking S draws from a multivariate normal which is the distribution of teh beta vector in asymptopia#
beta.sim <- mvrnorm(S, beta, vcmat)
X <- cbind(const,xx1)
lower <- 0.025*S#
upper <- 0.975*S#
#
#Sort to get the CI-bounds#
#
for (i in 1:sx){#
	min[i] <- sort(out[i,])[lower]#
	median.me[i] <- median(out[i,])#
	max[i] <- sort(out[i,])[upper]#
	}
lines(xx1, min, col="grey80", lty=2, lwd=1.9)#
lines(xx1, max, col="grey80", lty=2, lwd=1.9)#
lines(xx1, median.me, type="l", lwd=2.0)
rug(x1, ticksize=0.03, lwd=0.08, col="black")
#############################################################
#############################################################
##
#				4291 -- Limited Dependant Variables#
#			   Lab 3 (Jordan Kyle and Lucas Leemann)#
##
#############################################################
#############################################################
#
library(Rlab)#
library(mnormt)#
library(foreign)#
#
# Illustration: Write your own likelihood function, maximize it,#
# and use simulation to generate predicted probabilities incl. the#
# appropriate uncertainty#
#
#TRY FAKE DATA#
#
u <- rnorm(400)#
x1 <- rnorm(400)#
#
ylat <- 0.2 * x1  + u#
y <- rep(0, 400)#
#
y[ylat>0]<-1#
#
#
# Likelihood Probit as a Function in R#
#
llprobit <- function(b,x,y) {#
  y <- as.matrix(y)#
  x <- as.matrix(x)#
  onevec <- rep(1,nrow(x)) #
  x1 <- cbind(onevec,x)#
  xb <- x1 %*% b #
  return(-sum (y * (log(pnorm(xb))) + (1-y) * log(1-pnorm(xb)))) #
}#
#
xint <-  cbind(rep(1,400),x1)#
bols <- (solve(t(xint) %*% xint)) %*% (t(xint) %*% y)#
#
# estimate the probit model #
probit.out <- optim(bols,llprobit,x=x1,y=y,hessian = T)#
#
#Let's check our answers#
mod.probit <- glm(formula=y ~ x1,family=binomial(link="probit"))#
#
#Simulating betas#
#library(arm)#
beta <- probit.out$par#
vcmat <- solve(probit.out$hessian)#
#
tab <- array(NA, c(2,2))#
tab[,1] <- beta#
tab[,2] <- c(sqrt(vcmat[1,1]), sqrt(vcmat[2,2]))#
#
n.sim <- 1000#
beta.sim <- mvrnorm(n.sim, beta, vcmat)#
#
hist(beta.sim[,1], breaks=22)#
hist(beta.sim[,2], breaks=22)#
#
p.probit <- function(b,x){	##estimated probabilities for any value of x#
	b <- as.matrix(b)#
	x <- as.matrix(x)#
	const <- rep(1, dim(x)[1])#
	x <- cbind(const, x)#
	print(dim(x))#
	print(dim(b))#
	pnorm(b%*%t(x)) #
	} #
#
#
mfx2 <- function(b,x){#
	x <- cbind(1, x)#
	mf <- b[,-1]*dnorm(b%*%t(x))  ## marginal effect using derivative method#
	return(mf)#
	} #
#
#
pp <- p.probit(beta.sim, 0.5)#
hist(pp, breaks=28)#
hist(pp, breaks=16, xlim=c(0,1), col="grey95", main="Predicted Probability and its Uncertainty", xlab="Predicted Probability")#
#
mf <- mfx2(beta.sim,0.5)#
hist(mf, breaks=28, main="Histogram of Marginal Effects", col="grey95", xlab="Simulated Marginal Effects")#
#
#
#
# Now, let's do some prediction#
S <- 1000										# number of simulations#
sx <- 20#
xx1 <- seq(summary(x1)[1],summary(x1)[6],length.out=sx)  # the variable that will be on the x-axis#
const <- rep(1,sx)								# constant#
#
#Estimated beta and var(beta)#
beta <- probit.out$par#
vcv <- as.matrix(solve(probit.out$hessian))#
#
# Taking S draws from a multivariate normal which is the distribution of teh beta vector in asymptopia#
beta.sim <- mvrnorm(S, beta, vcmat)#
X <- cbind(const,xx1)#
#
#
#
out <- pnorm(X%*%t(beta.sim))#
#
min <- rep(NA,sx)#
max <- rep(NA,sx)#
median.me <- rep(NA,sx)#
#
# Adjust here to change the confidence level (here: 95%)#
lower <- 0.025*S#
upper <- 0.975*S#
#
#Sort to get the CI-bounds#
#
for (i in 1:sx){#
	min[i] <- sort(out[i,])[lower]#
	median.me[i] <- median(out[i,])#
	max[i] <- sort(out[i,])[upper]#
	}#
	#
# The final output#
plot(xx1,median.me, type="l", main="Predicted Probability with Uncertainty", ylab="Pred. Probability", xlab="Explanatory Variable of Interest",ylim=c(0,1), xlim=c(summary(x1)[1],summary(x1)[6]), lwd=2.5, col="black", frame.plot=FALSE) #
lines(xx1, min, col="grey80", lty=2, lwd=1)#
lines(xx1, max, col="grey80", lty=2, lwd=1)#
#
# Makes Graph nicer#
frame.y <- c(min,rev(max))#
frame.x <- c(xx1, rev(xx1))#
polygon(frame.x, frame.y, density=-1, col="grey97", border=NA)#
#
# Need to re-draw the lines#
lines(xx1, min, col="grey80", lty=2, lwd=1.9)#
lines(xx1, max, col="grey80", lty=2, lwd=1.9)#
lines(xx1, median.me, type="l", lwd=2.0)#
#
rug(x1, ticksize=0.03, lwd=0.08, col="black")
#############################################################
#############################################################
##
#				4291 -- Limited Dependant Variables#
#			   Lab 3 (Jordan Kyle and Lucas Leemann)#
##
#############################################################
#############################################################
#
#library(Rlab)#
library(mnormt)#
library(foreign)#
#
# Illustration: Write your own likelihood function, maximize it,#
# and use simulation to generate predicted probabilities incl. the#
# appropriate uncertainty#
#
#TRY FAKE DATA#
#
u <- rnorm(400)#
x1 <- rnorm(400)#
#
ylat <- 0.2 * x1  + u#
y <- rep(0, 400)#
#
y[ylat>0]<-1#
#
#
# Likelihood Probit as a Function in R#
#
llprobit <- function(b,x,y) {#
  y <- as.matrix(y)#
  x <- as.matrix(x)#
  onevec <- rep(1,nrow(x)) #
  x1 <- cbind(onevec,x)#
  xb <- x1 %*% b #
  return(-sum (y * (log(pnorm(xb))) + (1-y) * log(1-pnorm(xb)))) #
}#
#
xint <-  cbind(rep(1,400),x1)#
bols <- (solve(t(xint) %*% xint)) %*% (t(xint) %*% y)#
#
# estimate the probit model #
probit.out <- optim(bols,llprobit,x=x1,y=y,hessian = T)#
#
#Let's check our answers#
mod.probit <- glm(formula=y ~ x1,family=binomial(link="probit"))#
#
#Simulating betas#
#library(arm)#
beta <- probit.out$par#
vcmat <- solve(probit.out$hessian)#
#
tab <- array(NA, c(2,2))#
tab[,1] <- beta#
tab[,2] <- c(sqrt(vcmat[1,1]), sqrt(vcmat[2,2]))#
#
n.sim <- 1000#
beta.sim <- mvrnorm(n.sim, beta, vcmat)#
#
hist(beta.sim[,1], breaks=22)#
hist(beta.sim[,2], breaks=22)#
#
p.probit <- function(b,x){	##estimated probabilities for any value of x#
	b <- as.matrix(b)#
	x <- as.matrix(x)#
	const <- rep(1, dim(x)[1])#
	x <- cbind(const, x)#
	print(dim(x))#
	print(dim(b))#
	pnorm(b%*%t(x)) #
	} #
#
#
mfx2 <- function(b,x){#
	x <- cbind(1, x)#
	mf <- b[,-1]*dnorm(b%*%t(x))  ## marginal effect using derivative method#
	return(mf)#
	} #
#
#
pp <- p.probit(beta.sim, 0.5)#
hist(pp, breaks=28)#
hist(pp, breaks=16, xlim=c(0,1), col="grey95", main="Predicted Probability and its Uncertainty", xlab="Predicted Probability")#
#
mf <- mfx2(beta.sim,0.5)#
hist(mf, breaks=28, main="Histogram of Marginal Effects", col="grey95", xlab="Simulated Marginal Effects")#
#
#
#
# Now, let's do some prediction#
S <- 1000										# number of simulations#
sx <- 20#
xx1 <- seq(summary(x1)[1],summary(x1)[6],length.out=sx)  # the variable that will be on the x-axis#
const <- rep(1,sx)								# constant#
#
#Estimated beta and var(beta)#
beta <- probit.out$par#
vcv <- as.matrix(solve(probit.out$hessian))#
#
# Taking S draws from a multivariate normal which is the distribution of teh beta vector in asymptopia#
beta.sim <- mvrnorm(S, beta, vcmat)#
X <- cbind(const,xx1)#
#
#
#
out <- pnorm(X%*%t(beta.sim))#
#
min <- rep(NA,sx)#
max <- rep(NA,sx)#
median.me <- rep(NA,sx)#
#
# Adjust here to change the confidence level (here: 95%)#
lower <- 0.025*S#
upper <- 0.975*S#
#
#Sort to get the CI-bounds#
#
for (i in 1:sx){#
	min[i] <- sort(out[i,])[lower]#
	median.me[i] <- median(out[i,])#
	max[i] <- sort(out[i,])[upper]#
	}#
	#
# The final output#
plot(xx1,median.me, type="l", main="Predicted Probability with Uncertainty", ylab="Pred. Probability", xlab="Explanatory Variable of Interest",ylim=c(0,1), xlim=c(summary(x1)[1],summary(x1)[6]), lwd=2.5, col="black", frame.plot=FALSE) #
lines(xx1, min, col="grey80", lty=2, lwd=1)#
lines(xx1, max, col="grey80", lty=2, lwd=1)#
#
# Makes Graph nicer#
frame.y <- c(min,rev(max))#
frame.x <- c(xx1, rev(xx1))#
polygon(frame.x, frame.y, density=-1, col="grey97", border=NA)#
#
# Need to re-draw the lines#
lines(xx1, min, col="grey80", lty=2, lwd=1.9)#
lines(xx1, max, col="grey80", lty=2, lwd=1.9)#
lines(xx1, median.me, type="l", lwd=2.0)#
#
rug(x1, ticksize=0.03, lwd=0.08, col="black")
tab
#############################################################
#############################################################
##
#				4291 -- Limited Dependant Variables#
#			   Lab 3 (Jordan Kyle and Lucas Leemann)#
##
#############################################################
#############################################################
#
#
library(mnormt)#
library(foreign)#
#
# Illustration: Write your own likelihood function, maximize it,#
# and use simulation to generate predicted probabilities incl. the#
# appropriate uncertainty#
#
#TRY FAKE DATA#
u <- rnorm(400)#
x1 <- rnorm(400)#
#
ylat <- 0.2 * x1  + u#
y <- rep(0, 400)#
#
y[ylat>0]<-1#
#
#
# Likelihood Probit as a Function in R#
#
llprobit <- function(b,x,y) {#
  y <- as.matrix(y)#
  x <- as.matrix(x)#
  onevec <- rep(1,nrow(x)) #
  x1 <- cbind(onevec,x)#
  xb <- x1 %*% b #
  return(-sum (y * (log(pnorm(xb))) + (1-y) * log(1-pnorm(xb)))) #
}#
#
xint <-  cbind(rep(1,400),x1)#
bols <- (solve(t(xint) %*% xint)) %*% (t(xint) %*% y)#
#
# estimate the probit model #
probit.out <- optim(bols,llprobit,x=x1,y=y,hessian = T)#
#
#Let's check our answers#
mod.probit <- glm(formula=y ~ x1,family=binomial(link="probit"))#
#
#Simulating betas#
beta <- probit.out$par#
vcmat <- solve(probit.out$hessian)#
#
#tab <- array(NA, c(2,2))#
#tab[,1] <- beta#
#tab[,2] <- c(sqrt(vcmat[1,1]), sqrt(vcmat[2,2]))#
#
n.sim <- 1000#
beta.sim <- mvrnorm(n.sim, beta, vcmat)#
#
hist(beta.sim[,1], breaks=22)#
hist(beta.sim[,2], breaks=22)#
#
p.probit <- function(b,x){	##estimated probabilities for any value of x#
	b <- as.matrix(b)#
	x <- as.matrix(x)#
	const <- rep(1, dim(x)[1])#
	x <- cbind(const, x)#
	print(dim(x))#
	print(dim(b))#
	pnorm(b%*%t(x)) #
	} #
#
#
mfx2 <- function(b,x){#
	x <- cbind(1, x)#
	mf <- b[,-1]*dnorm(b%*%t(x))  ## marginal effect using derivative method#
	return(mf)#
	} #
#
#
pp <- p.probit(beta.sim, 0.5)#
hist(pp, breaks=28)#
hist(pp, breaks=16, xlim=c(0,1), col="grey95", main="Predicted Probability and its Uncertainty", xlab="Predicted Probability")#
#
mf <- mfx2(beta.sim,0.5)#
hist(mf, breaks=28, main="Histogram of Marginal Effects", col="grey95", xlab="Simulated Marginal Effects")#
#
#
#
# Now, let's do some prediction#
S <- 1000										# number of simulations#
sx <- 20#
xx1 <- seq(summary(x1)[1],summary(x1)[6],length.out=sx)  # the variable that will be on the x-axis#
const <- rep(1,sx)								# constant#
#
#Estimated beta and var(beta)#
beta <- probit.out$par#
vcv <- as.matrix(solve(probit.out$hessian))#
#
# Taking S draws from a multivariate normal which is the distribution of teh beta vector in asymptopia#
beta.sim <- mvrnorm(S, beta, vcmat)#
X <- cbind(const,xx1)#
#
#
#
out <- pnorm(X%*%t(beta.sim))#
#
min <- rep(NA,sx)#
max <- rep(NA,sx)#
median.me <- rep(NA,sx)#
#
# Adjust here to change the confidence level (here: 95%)#
lower <- 0.025*S#
upper <- 0.975*S#
#
#Sort to get the CI-bounds#
#
for (i in 1:sx){#
	min[i] <- sort(out[i,])[lower]#
	median.me[i] <- median(out[i,])#
	max[i] <- sort(out[i,])[upper]#
	}#
	#
# The final output#
plot(xx1,median.me, type="l", main="Predicted Probability with Uncertainty", ylab="Pred. Probability", xlab="Explanatory Variable of Interest",ylim=c(0,1), xlim=c(summary(x1)[1],summary(x1)[6]), lwd=2.5, col="black", frame.plot=FALSE) #
lines(xx1, min, col="grey80", lty=2, lwd=1)#
lines(xx1, max, col="grey80", lty=2, lwd=1)#
#
# Makes Graph nicer#
frame.y <- c(min,rev(max))#
frame.x <- c(xx1, rev(xx1))#
polygon(frame.x, frame.y, density=-1, col="grey97", border=NA)#
#
# Need to re-draw the lines#
lines(xx1, min, col="grey80", lty=2, lwd=1.9)#
lines(xx1, max, col="grey80", lty=2, lwd=1.9)#
lines(xx1, median.me, type="l", lwd=2.0)#
#
rug(x1, ticksize=0.03, lwd=0.08, col="black")
n.sim <- 1000#
beta.sim <- mvrnorm(n.sim, beta, vcmat)#
#
hist(beta.sim[,1], breaks=22)#
hist(beta.sim[,2], breaks=22)
hist(beta.sim[,1], breaks=22)
hist(beta.sim[,2], breaks=22)
p.probit <- function(b,x){	##estimated probabilities for any value of x#
	b <- as.matrix(b)#
	x <- as.matrix(x)#
	const <- rep(1, dim(x)[1])#
	x <- cbind(const, x)#
	print(dim(x))#
	print(dim(b))#
	pnorm(b%*%t(x)) #
	}
mfx2 <- function(b,x){#
	x <- cbind(1, x)#
	mf <- b[,-1]*dnorm(b%*%t(x))  ## marginal effect using derivative method#
	return(mf)#
	}
pp <- p.probit(beta.sim, 0.5)#
hist(pp, breaks=28)#
hist(pp, breaks=16, xlim=c(0,1), col="grey95", main="Predicted Probability and its Uncertainty", xlab="Predicted Probability")
pp <- p.probit(beta.sim, 0.5)#
hist(pp, breaks=28, col="grey95", main="Predicted Probability and its Uncertainty", xlab="Predicted Probability")
mf <- mfx2(beta.sim,0.5)#
hist(mf, breaks=28, main="Histogram of Marginal Effects", col="grey95", xlab="Simulated Marginal Effects")
S <- 1000										# number of simulations#
sx <- 20#
xx1 <- seq(summary(x1)[1],summary(x1)[6],length.out=sx)  # the variable that will be on the x-axis#
const <- rep(1,sx)								# constant#
#
#Estimated beta and var(beta)#
beta <- probit.out$par#
vcv <- as.matrix(solve(probit.out$hessian))
beta vector in asymptopia#
beta.sim <- mvrnorm(S, beta, vcmat)#
X <- cbind(const,xx1)
beta.sim <- mvrnorm(S, beta, vcmat)#
X <- cbind(const,xx1)
# Taking S draws from a multivariate normal which is the distribution of the #
# beta vector in asymptopia#
beta.sim <- mvrnorm(S, beta, vcmat)#
X <- cbind(const,xx1)
out <- pnorm(X%*%t(beta.sim))
min <- rep(NA,sx)#
max <- rep(NA,sx)#
median.me <- rep(NA,sx)
lower <- 0.025*S#
upper <- 0.975*S
for (i in 1:sx){#
	min[i] <- sort(out[i,])[lower]#
	median.me[i] <- median(out[i,])#
	max[i] <- sort(out[i,])[upper]#
	}
plot(xx1,median.me, type="l", main="Predicted Probability with Uncertainty", ylab="Pred. Probability", xlab="Explanatory Variable of Interest",ylim=c(0,1), xlim=c(summary(x1)[1],summary(x1)[6]), lwd=2.5, col="black", frame.plot=FALSE) #
lines(xx1, min, col="grey80", lty=2, lwd=1)#
lines(xx1, max, col="grey80", lty=2, lwd=1)
frame.y <- c(min,rev(max))#
frame.x <- c(xx1, rev(xx1))#
polygon(frame.x, frame.y, density=-1, col="grey97", border=NA)
 Need to re-draw the lines#
lines(xx1, min, col="grey80", lty=2, lwd=1.9)#
lines(xx1, max, col="grey80", lty=2, lwd=1.9)#
lines(xx1, median.me, type="l", lwd=2.0)
#
rug(x1, ticksize=0.03, lwd=0.08, col="black")
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 100#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1)
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))
## b.range <- as.matrix(seq(-2,2,.01))#
#
b.range <- as.matrix(seq(-2,2,.01))
z <- .2 + b.range %*% t(x1)
z <- .2 + b.range %*% t(x1)#
#
## compute the likelihood profile#
#
ll.profile <- matrix(NA,length(b.range),1)
for(s in 1:length(b.range)) {#
#
  ll.profile[s] <- sum(y*log(pnorm(z[s,])) + (1-y)*log(1-pnorm(z[s,])))#
  #
}
plot(b.range,ll.profile,type="l",xlab="beta",ylab="log-likelihood value")
z1 <- cbind(1,x1) %*% b  ## the z-score for true parameter values
ll.val1 <- sum(y*log(pnorm(z1)) + (1-y)*log(1-pnorm(z1)))
#
points(1,ll.val1)
plot(b.range,ll.profile,type="l",xlab="beta",ylab="log-likelihood value")#
#
## plot a point on the the likelihood function#
#
z1 <- cbind(1,x1) %*% b  ## the z-score for true parameter values#
#
ll.val1 <- sum(y*log(pnorm(z1)) + (1-y)*log(1-pnorm(z1)))#
#
points(1,ll.val1)
n <- 1000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}#
#
#
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=.5)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))#
#
out$par
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 1000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}#
#
#
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=.1)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 1000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}#
#
#
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=.5)
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=1)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))
n <- 5000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}#
#
#
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=1)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=.5)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=.1)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))
deriv2(out$par, x1,y)
out$hessian
x <- x1#
#
x <- as.matrix(x)#
	y <- as.vector(y)
b <- c(.2,1)
x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)
b <- out$par
x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))
x <- x1#
b <- c(.2,1)#
b <- out$par#
x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes
x <- x1#
b <- c(.2,1)#
#b <- out$par#
x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 5000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}#
#
#
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=.1)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))#
#
out$par
x <- x1#
b <- c(.2,1)#
b <- out$par#
x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes
d
# Fake Data#
D <- 1000#
#
u <- rnorm(D)#
x1 <- rnorm(D)#
#
#
ylat <- 0.2 * x1  + u#
y <- rep(0, D)#
#
y[ylat>0]<-1#
#
# Probit#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# Optimize it#
stv <- c(0,0)#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## 1st Derivative#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint#
	return(d)#
}#
#
deriv1(b=out$par, x=x1, y=y)
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# Optimize it#
stv <- c(0,0)#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint#
	return(d)#
}#
#
deriv1(b=out$par, x=x1, y=y)
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}
deriv1(out$par, x1,y)
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint#
	return(d)#
}#
#
deriv1(b=out$par, x=x1, y=y)
#
#
##################################################################
##################################################################
##
#	Derivatives and Hill Climbing - Lucas Leemann - 10/1/2010#
##
##################################################################
##################################################################
#
#
# Fake Data#
D <- 1000#
#
u <- rnorm(D)#
x1 <- rnorm(D)#
#
#
ylat <- 0.2 * x1  + u#
y <- rep(0, D)#
#
y[ylat>0]<-1#
#
# Probit#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# Optimize it#
stv <- c(0,0)#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## 1st Derivative#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint#
	return(d)#
}#
#
deriv1(b=out$par, x=x1, y=y)
D <- 1000#
#
u <- rnorm(D)#
x1 <- rnorm(D)#
#
#
ylat <- 0.999 * x1  + u#
y <- rep(0, D)#
#
y[ylat>0]<-1#
#
# Probit#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# Optimize it#
stv <- c(0,0)#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## 1st Derivative#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint#
	return(d)#
}#
#
deriv1(b=out$par, x=x1, y=y)
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 5000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}#
#
#
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=1.0)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))#
#
out$par#
#
## create a range of beta values#
#
## b.range <- as.matrix(seq(-2,2,.01))#
#
b.range <- as.matrix(seq(-2,2,.01))#
#
## create a range of z scores for profiling likelihood#
#
z <- .2 + b.range %*% t(x1)#
#
## compute the likelihood profile#
#
ll.profile <- matrix(NA,length(b.range),1)#
#
for(s in 1:length(b.range)) {#
#
  ll.profile[s] <- sum(y*log(pnorm(z[s,])) + (1-y)*log(1-pnorm(z[s,])))#
  #
}#
#
## plot the likelihood for the range of beta values#
#
plot(b.range,ll.profile,type="l",xlab="beta",ylab="log-likelihood value")#
#
## plot a point on the the likelihood function#
#
z1 <- cbind(1,x1) %*% b  ## the z-score for true parameter values#
#
ll.val1 <- sum(y*log(pnorm(z1)) + (1-y)*log(1-pnorm(z1)))#
#
points(1,ll.val1)#
#
#
deriv1(out$par, x1,y)
deriv1(out$par, x1,y)#
deriv1(c(.2,1), x1,y)
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 5000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
#
## function for computing 2nd derivatives#
#
deriv2 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	phi <- dnorm(xb)#
	Phi <- pnorm(xb)	#
	B <- y*(phi+Phi*xb)/Phi^2+(1-y)*(phi-(1-Phi)*xb)/(1-Phi)^2 ## from p. 55 in notes#
	X <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
		X[,,j] <- xint[j,]%*%t(xint[j,])#
		}#
	d <- array(NA,c(2,2,length(y)))#
	for (j in 1:length(y)){#
	d[,,j] <- - phi[j]*B[j]*X[,,j]#
		}#
	D <- matrix(NA,2,2)#
	D[1,1] <- sum(d[1,1,])#
	D[2,1] <- sum(d[2,1,]) #
	D[1,2] <- sum(d[1,2,])#
	D[2,2] <- sum(d[2,2,])#
	return(D)#
}#
#
## try the function with particular values#
#
deriv2(b=c(0,0),x=x1,y=y)#
#
out$hessian#
#
## hill climbing algorithm#
#
hill <- function(theta0, der1, der2, y, x, tolerance=10^-7, stepsize=1, digits=5){#
	a <- tolerance#
	s <- stepsize#
	d <- digits#
	theta0 <- as.vector(theta0)#
	y <- as.vector(y)#
	x <- as.vector(x)#
	counter <-0#
	A<-c(-9999,-9999)#
		while((sum(abs(theta0-A)>a))) {#
		A <- theta0#
		theta1 <- theta0 + s*der1(theta0,x,y)%*%solve(-der2(theta0,x,y)) ## eqn. 3.5 from notes#
                cat(theta1,"\n") ## print values at each iteration#
		theta0 <- theta1#
				}#
		theta0 <- round(theta0, d)#
		print(c("Beta is:",theta0))#
		return(theta0)#
	}#
#
#
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=1.0, tolerance=10^-9)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))#
#
out$par#
#
## create a range of beta values#
#
## b.range <- as.matrix(seq(-2,2,.01))#
#
b.range <- as.matrix(seq(-2,2,.01))#
#
## create a range of z scores for profiling likelihood#
#
z <- .2 + b.range %*% t(x1)#
#
## compute the likelihood profile#
#
ll.profile <- matrix(NA,length(b.range),1)#
#
for(s in 1:length(b.range)) {#
#
  ll.profile[s] <- sum(y*log(pnorm(z[s,])) + (1-y)*log(1-pnorm(z[s,])))#
  #
}#
#
## plot the likelihood for the range of beta values#
#
plot(b.range,ll.profile,type="l",xlab="beta",ylab="log-likelihood value")#
#
## plot a point on the the likelihood function#
#
z1 <- cbind(1,x1) %*% b  ## the z-score for true parameter values#
#
ll.val1 <- sum(y*log(pnorm(z1)) + (1-y)*log(1-pnorm(z1)))#
#
points(1,ll.val1)#
#
#
deriv1(out$par, x1,y)
start.time <- date()#
#
your.beta <- hill(theta0=c(0,0), der1=deriv1, der2=deriv2, y=y, x=x1, stepsize=1.0, tolerance=10^-7)#
#
end.time <- date()#
#
cat(c("Job started at:",start.time,"\n"))#
#
cat(c("Job finished at:",end.time,"\n"))
out$par
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 5000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
deriv1(out$par, x1,y)
##################################################################
##################################################################
##
#	Derivatives and hill climbing for probit MLE #
##
##################################################################
##################################################################
#
#
## DGP#
#
n <- 5000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
deriv1(out$par, x1,y)#
out$par
#
n <- 1000#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
deriv1(out$par, x1,y)
n <- 100#
#
u <- rnorm(n)#
#
x1 <- as.matrix(rnorm(n))#
#
b <- c(0.2, 1) #
#
ystar <- cbind(1,x1) %*% b + u#
#
y <- rep(0, n)#
#
y[ystar>0]<-1#
#
# probit #
#
llik.probit <- function(y,x,b) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	int <- rep(1,length(y))#
	xint <- cbind(int,x)#
	xb <- xint%*%b#
	p <- pnorm(xb)#
	ll <- sum(y*log(p)) + sum((1-y)*log(1-p))#
 	return(ll)#
	}#
#
# run optim to compute MLEs#
#
stv <- c(0,0)#
#
out <-optim(stv,llik.probit,x=x1,y=y, method="BFGS", hessian=TRUE, control=list(fnscale=-1,trace=5))#
#
#
## function for computing 1st derivatives#
#
deriv1 <- function(b,x,y) {#
	x <- as.matrix(x)#
	y <- as.vector(y)#
	b <- as.vector(b)#
	xint <- cbind(1,x)#
	xb <- xint%*%b#
	d.sc1 <- (y-pnorm(xb))*dnorm(xb)#
	d.sc2 <- pnorm(xb)*(1-pnorm(xb))#
	d.sc <- d.sc1/d.sc2#
	d <- t(d.sc)%*%xint ## see p. 55 in notes#
	return(d)#
}#
#
## try the function with particular values#
#
deriv1(b=c(0,0), x=x1, y=y)#
deriv1(out$par, x1,y)
#Daten, mit statistischem Mittelwert und Standard Abweichung:#
#
#zu wenig (bis 01-03 und 04-08):#
data_w03 <- c(56.68,65.47,63.49,3.25,54.7,57.94,63.1,47.69,59.55,65.42,47.62,64.81,45.96,61.01,58.51,53.32,67.01,61.97,57.3,33.33,60,64.93,64.71,46.51,59.95,64.42,55.68,69.35,69.24,66.36,45.48,55.29)#
data_w08 <- c(62.24,67.87,73.91,5.04,82.26,70.13,65.6,49.74,61.01,77.63,54.44,58.23,50.27,65.14,65.15,51.93,66.13,68.67,67.62,69.4,80.7,61.54,72.25,46.12,56.89,63,64,65,71.3,69.82,38.11)#
#
m_w03 = mean(data_w03)#
d_w03 = sd(data_w03) #
m_w08 = mean(data_w08)#
d_w08 = sd(data_w08) #
#
#zu viel (bis 01-03 und 04-08):#
data_v03 <- c(76.97,91.18,85.22,79.4,92.85,88.67,90.48,77.78,85.35,78.33,82.14,80.15,82.67,75.51,85.47,81.39,79.31,98.25,79.36,84.52,77.93,100)#
data_v08 <- c(72.75,90.52,84.13,69.32,94.5,91.88,85.27,76.59,91.08,81.52,78.58,74.83,82.58,74.72,88.5,82.01,89.52,86.34,80.6,88.09,86.53,100)#
#
m_v03 = mean(data_v03)#
d_v03 = sd(data_v03) #
m_v08 = mean(data_v08)#
d_v08 = sd(data_v08) #
#
#genau richtig (bis 01-03 und 04-08):#
data_r03 <- c(72.09,75.23,70.89,73.71,73.9,73.14,72.29,71.43,73.52,70.87,70.96,72.22)#
data_r08 <- c(67.58,71.61,72.76,78.83,74.36,69.53,78.76,71.79,72.55,71.45,81.32,78)#
#
m_r03 = mean(data_r03)#
d_r03 = sd(data_r03) #
m_r08 = mean(data_r08)#
d_r08 = sd(data_r08) #
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%#
# Q-Q plot#
x <- seq(0, 100, by=.01)#
#
#--------------------------------------------------------------------------------------------------------------------#
#zu wenig Deutsch:#
#
#Speichern der Grafik als .ps file:#
postscript(file="plot_zuwenigDeutsch.ps",horizontal=TRUE,onefile=FALSE,print.it=FALSE)#
par(mfrow=c(2,2))#
# freq = FALSE: normalisiert die Flaeche des Histogramms zu '1' (Approximation der Dichte)#
# breaks=50: anzahl slots#
#xlim=c(0,100): range der x-Achse im Histogramm#
hist(data_w03,freq=FALSE,breaks=50,xlim=c(0,100),xlab=paste("% Deutsch 01-03, p Wert ks Test = ",format(ks_w03_pvalue,digits=3)),#
ylab="Dichte",main="Histogramm: zu wenig Deutsch 01-03")#
#Dichtefunktion der Normalverteilung mit gegebenem mean und sd, ausgewertet auf den Werten in x:#
dens_w03 <- dnorm(x,m_w03,d_w03)#
#zeichnet die Linie ins Histogramm (blau, Dicke 2):#
lines(x,dens_w03,col="blue",lwd=2)#
#
#qq Plot (mit Normalverteilung): sollte eine Linie von links unten nach rechts oben sein...#
qqnorm(data_w03)#
qqline(data_w03,col=2)#
#
#w08:#
# freq = FALSE: normalisiert die Flaeche des Histogramms zu '1' (Approximation der Dichte)#
# breaks=50: anzahl slots#
#xlim=c(0,100): range der x-Achse im Histogramm#
hist(data_w08,freq=FALSE,breaks=50,xlim=c(0,100),xlab=paste("% Deutsch 04-08, p Wert ks Test = ",format(ks_w08_pvalue,digits=3)),#
ylab="Dichte",main="Histogramm: Daten 04-08")#
#Dichtefunktion der Normalverteilung mit gegebenem mean und sd, ausgewertet auf den Werten in x:#
dens_w08 <- dnorm(x,m_w08,d_w08)#
#zeichnet die Linie ins Histogramm (blau, Dicke 2):#
lines(x,dens_w08,col="blue",lwd=2)#
#
#qq Plot (mit Normalverteilung): sollte eine Linie von links unten nach rechts oben sein...#
qqnorm(data_w08)#
qqline(data_w08,col=2)
# breaks=50: anzahl slots#
#xlim=c(0,100): range der x-Achse im Histogramm#
hist(data_w03,freq=FALSE,breaks=50,xlim=c(0,100),xlab=paste("% Deutsch 01-03, p Wert ks Test = ",format(ks_w03_pvalue,digits=3)),#
ylab="Dichte",main="Histogramm: zu wenig Deutsch 01-03")#
#Dichtefunktion der Normalverteilung mit gegebenem mean und sd, ausgewertet auf den Werten in x:#
dens_w03 <- dnorm(x,m_w03,d_w03)#
#zeichnet die Linie ins Histogramm (blau, Dicke 2):#
lines(x,dens_w03,col="blue",lwd=2)
hist(data_w03,freq=FALSE,breaks=50,xlim=c(0,100),xlab=paste("% Deutsch 01-03, p Wert ks Test = "),#
ylab="Dichte",main="Histogramm: zu wenig Deutsch 01-03")#
#Dichtefunktion der Normalverteilung mit gegebenem mean und sd, ausgewertet auf den Werten in x:#
dens_w03 <- dnorm(x,m_w03,d_w03)#
#zeichnet die Linie ins Histogramm (blau, Dicke 2):#
lines(x,dens_w03,col="blue",lwd=2)
dev.off()
hist(data_w03,freq=FALSE,breaks=50,xlim=c(0,100),xlab=paste("% Deutsch 01-03, p Wert ks Test = "),#
ylab="Dichte",main="Histogramm: zu wenig Deutsch 01-03")#
#Dichtefunktion der Normalverteilung mit gegebenem mean und sd, ausgewertet auf den Werten in x:#
dens_w03 <- dnorm(x,m_w03,d_w03)
lines(x,dens_w03,col="blue",lwd=2)
qqnorm(data_w03)
qqline(data_w03,col=2)
help(histogram)
help(histo)
help(hist)
dat5 <- read.dta("/Users/lucas/Dropbox/icpsr/zorn_2011/exercises/Exercise5/Exercise5.dta")
library(car)#
library(foreign)#
library(xtable)#
library(VGAM)#
library(mlogit)#
library(MNP)#
library(sfsmisc)#
#
#
dat5 <- read.dta("/Users/lucas/Dropbox/icpsr/zorn_2011/exercises/Exercise5/Exercise5.dta")
some(dat5)
sum.1 <- summary(dat5)
print(sum.1, floating=FALSE)
(counts <- table(data.1$cartype))
(counts <- table(dat5$cartype))
barplot(counts, xlab="Vehicle type", main="")
xtable(sum.1, digit=3)
# listwise deletion#
data.1 <- na.omit(dat5)
mlogit2 <- vglm(cartype ~ education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, multinomial, data=data.1)#
summary(mlogit2)
m.logit.1 <- mlogit2
#
PickCar <- ifelse(fitted.values(m.logit.1)[, 1] > fitted.values(m.logit.1)[, 2] &#
                  fitted.values(m.logit.1)[, 1] > fitted.values(m.logit.1)[, 3], 1, 0)#
#
PickSUV <- ifelse(fitted.values(m.logit.1)[, 2] > fitted.values(m.logit.1)[, 1] &#
                  fitted.values(m.logit.1)[, 2] > fitted.values(m.logit.1)[, 3], 2, 0)#
#
PickPickup <- ifelse(fitted.values(m.logit.1)[, 3] > fitted.values(m.logit.1)[, 1] &#
                     fitted.values(m.logit.1)[, 3] > fitted.values(m.logit.1)[, 2], 3, 0)#
#
OutHat <- PickCar + PickSUV + PickPickup#
table(data.1$cartype, OutHat)
mlogit1 <- vglm(cartype ~ education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, multinomial, data=data.1)#
summary(mlogit1)
predict(mlogit1)
help(predict)
predict.vgml(mlogit1, type="response")
predict.vglm(mlogit1, type="response")
PICK <- 0#
PICK[(predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3])] <- 1
predPROB[,1]
predPROB[,2]
(predPROB[,1]>predPROB[,2])
(predPROB[,1]>predPROB[,3])
predPROB <- predict.vglm(mlogit1, type="response")#
#
PICK <- 0#
PICK[(predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3])] <- 1
(predPROB[,2]>predPROB[,2])&(predPROB[,2]>predPROB[,3])
PICK <- rep(NA,length(predPROB)#
PICK[(predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3])] <- 1
PICK <- rep(NA,length(predPROB)
#
predPROB <- predict.vglm(mlogit1, type="response")#
#
PICK <- rep(NA,length(predPROB))
PICK[(predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3])] <- 1
PICK[(predPROB[,2]>predPROB[,2])&(predPROB[,2]>predPROB[,3])] <- 2
### HW 5#
#
rm(list=ls(all=TRUE))#
#
library(car)#
library(foreign)#
library(xtable)#
library(VGAM)#
library(mlogit)#
library(MNP)#
library(sfsmisc)#
#
#
dat5 <- read.dta("/Users/lucas/Dropbox/icpsr/zorn_2011/exercises/Exercise5/Exercise5.dta")#
#
#
## 1#
some(dat5)#
summary(dat5)#
#
# listwise deletion#
data.1 <- na.omit(dat5)#
attach(data.1)#
mlogit1 <- vglm(cartype ~ education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, multinomial, data=data.1)#
summary(mlogit1)#
#
#
## 2#
#
predPROB <- predict.vglm(mlogit1, type="response")
length(predPROB)
table(predPROB)
predPROB
which((predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3]))
### HW 5#
#
rm(list=ls(all=TRUE))#
#
library(car)#
library(foreign)#
library(xtable)#
library(VGAM)#
library(mlogit)#
library(MNP)#
library(sfsmisc)#
#
#
dat5 <- read.dta("/Users/lucas/Dropbox/icpsr/zorn_2011/exercises/Exercise5/Exercise5.dta")#
#
#
## 1#
some(dat5)#
summary(dat5)#
#
# listwise deletion#
data.1 <- na.omit(dat5)#
attach(data.1)#
mlogit1 <- vglm(cartype ~ education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, multinomial)
summary(mlogit1)
#
predPROB <- predict.vglm(mlogit1, type="response")
length(which((predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3])))
length(which((predPROB[,2]>predPROB[,1])&(predPROB[,2]>predPROB[,3])))
length(which((predPROB[,3]>predPROB[,1])&(predPROB[,3]>predPROB[,2])))
dim(data.1)
PICK <- rep(NA,length(predPROB))#
PICK[(predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3])] <- 1#
PICK[(predPROB[,2]>predPROB[,2])&(predPROB[,2]>predPROB[,3])] <- 2
predPROB <- predict.vglm(mlogit1, type="response")#
#
PICK <- rep(NA,length(predPROB))#
PICK[(predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3])] <- 1#
PICK[(predPROB[,2]>predPROB[,2])&(predPROB[,2]>predPROB[,3])] <- 2#
PICK[(predPROB[,3]>predPROB[,1])&(predPROB[,3]>predPROB[,2])] <- 3
predPROB <- predict.vglm(mlogit1, type="response")#
#
PICK <- rep(NA,length(predPROB))#
PICK[which((predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3]))] <- 1#
PICK[which((predPROB[,2]>predPROB[,2])&(predPROB[,2]>predPROB[,3]))] <- 2#
PICK[which((predPROB[,3]>predPROB[,1])&(predPROB[,3]>predPROB[,2]))] <- 3
predPROB <- predict.vglm(mlogit1, type="response")#
#
PICK <- rep(NA,length(predPROB))#
PICK[which((predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3]))] <- 1
PICK <- rep(NA,dim(predPROB)[2])#
PICK[which((predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3]))] <- 1
PICK[which((predPROB[,2]>predPROB[,2])&(predPROB[,2]>predPROB[,3]))] <- 2
PICK
which((predPROB[,2]>predPROB[,2])&(predPROB[,2]>predPROB[,3]))
#
predPROB <- predict.vglm(mlogit1, type="response")#
#
PICK <- rep(NA,dim(predPROB)[2])#
PICK[which((predPROB[,1]>predPROB[,2])&(predPROB[,1]>predPROB[,3]))] <- 1#
PICK[which((predPROB[,2]>predPROB[,1])&(predPROB[,2]>predPROB[,3]))] <- 2#
PICK[which((predPROB[,3]>predPROB[,1])&(predPROB[,3]>predPROB[,2]))] <- 3
table(PICK)
table(PICK, cartype)
diag(table(PICK, cartype))
sum(diag(table(PICK, cartype)))/sum(table(PICK, cartype))
max(table(PICK, cartype))
max(table(PICK, cartype))/sum(table(PICK, cartype))
hats <- as.data.frame(fitted.values(m.logit.1))#
names(hats)[3] <- "Pickup"#
names(hats)[2] <- "SUV"#
names(hats)[1] <- "Car"#
#
attach(hats)#
scatterplotMatrix(~ Car + SUV + Pickup,#
                  diagonal="histogram", col=c("grey90", "grey30"))#
detach(hats)
hats <- as.data.frame(fitted.values(mlogit1))#
names(hats)[3] <- "Pickup"#
names(hats)[2] <- "SUV"#
names(hats)[1] <- "Car"#
#
attach(hats)#
scatterplotMatrix(~ Car + SUV + Pickup,#
                  diagonal="histogram", col=c("grey90", "grey30"))#
detach(hats)
data.2 <- cbind(data.1, hats)#
mat.1 <- matrix(NA, length(data.2$Car[data.2$GOP==1]), 6)#
head(mat.1)
head(mat.1)
#
mat.1[1:length(data.2$Car[data.2$democrat==1]), 1] <- data.2$Car[data.2$democrat==1]#
mat.1[1:length(data.2$Car[data.2$GOP==1]), 2] <- data.2$Car[data.2$GOP==1]#
mat.1[1:length(data.2$SUV[data.2$democrat==1]), 3] <- data.2$SUV[data.2$democrat==1]#
mat.1[1:length(data.2$SUV[data.2$GOP==1]), 4] <- data.2$SUV[data.2$GOP==1]#
mat.1[1:length(data.2$Pickup[data.2$democrat==1]), 5] <- data.2$Pickup[data.2$democrat==1]#
mat.1[1:length(data.2$Pickup[data.2$GOP==1]), 6] <- data.2$Pickup[data.2$GOP==1]#
#
boxplot.matrix(as.matrix(mat.1), names=c("Car, D", "Car, R", "SUV, D", "SUV, R",#
               "Pick, D", "Pick, R"), ylab="Predicted Probability", ylim=c(0, 1), main="")
#
mlogit2 <- vglm(cartype ~ education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, multinomial(refLevel=1), data=data.1)#
summary(mlogit2)
mlogit3 <- vglm(cartype ~ education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, multinomial(refLevel=2), data=data.1)#
summary(mlogit3)
data.1.Long <- mlogit.data(data.1, choice="cartype", shape="wide")
m.logit.4 <- mlogit(cartype ~ 1 | education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, data=data.1.Long, reflevel="Car")
data.1.Long <- mlogit.data(data.1, choice="cartype", shape="wide")#
#
m.logit.4 <- mlogit(cartype ~ 1 | education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, data=data.1.Long, reflevel="Car")#
#
m.logit.4noPU <- mlogit(cartype ~ 1 | education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, data=data.1.Long, reflevel="Car", alt.subset=c("Car", "SUV"))#
#
hmftest(m.logit.4, m.logit.4R)
hmftest(m.logit.4, m.logit.4noPU)
#
m.logit.4noSU <- mlogit(cartype ~ 1 | education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, data=data.1.Long, reflevel="Car", alt.subset=c("Car", "Pickup"))
m.logit.4noCA <- mlogit(cartype ~ 1 | education + age + urban + married + kids + black + female + democrat + GOP + bushapproval, data=data.1.Long, reflevel="Car", alt.subset=c("Pickup", "SUV"))
# IIA test (Pickup is dropped)#
hmftest(m.logit.4, m.logit.4noPU)#
#
# IIA test (SUV is dropped)#
hmftest(m.logit.4, m.logit.4noSU)#
#
# IIA test (Car is dropped)#
hmftest(m.logit.4, m.logit.4noCA)
m.probit.1 <- mnp(cartype ~ education + age + urban + married + kids + black + female +  democrat + GOP + bushapproval)
m.probit.1 <- mnp(cartype ~ education + age)
m.probit.1
m.probit.1 <- mnp(cartype ~ education + age + urban + married + kids + black + female +  democrat + GOP + bushapproval, draws=10000)
help(mnp)
#
m.probit.1 <- mnp(cartype ~ education + age + urban + married + kids + black + female +  democrat + GOP + bushapproval, n.draws=10000)
#
m.probit.1 <- mnp(cartype ~ education + age + urban + married + kids + black + female +  democrat + GOP + bushapproval, n.draws=100, verbose=TRUE)
m.probit.1 <- mnp(cartype ~ education + age + urban + married + kids + black + female +  democrat + GOP + bushapproval, n.draws=1000, verbose=TRUE)
#
m.probit.1 <- mnp(cartype ~ education + age + urban + married + kids + black + female +  democrat + GOP + bushapproval, n.draws=10000, verbose=TRUE)
#
m.probit.1 <- mnp(cartype ~ education + age + urban + married + kids + black + female +  democrat + GOP + bushapproval, n.draws=1000, verbose=TRUE)
summary(m.probit.1)
## Set seed using the standard seed for the whole book#
set.seed(1234567)#
#
## Generate three normal distributions, using n=100000#
prior <- rnorm(100000, mean = 0, sd = 2)#
posterior <- rnorm(100000, mean = 8, sd = .89)#
experimental <- rnorm(100000, mean = 10, sd = 1)
plot(experimental)
plot(density(experimental))
x <- seq(-10:15,length.out=100000)
hist(x)
x <- seq(-10,15,length.out=100000)
plot(x,dnorm(x,10,1))
w<-2
v<-2
if (w==2&v==2) print("fuck you")
if (length(w)==1&&v==2) print("fuck you")
resresplot <-#
function(Y,Z,X,prob,scale=1) {#
	w = Z/prob + (1-Z)/(1-prob)#
	w = w/mean(w)#
	Yres <- Y - lm(Y~X,weights=w)$fitted#
	Zres <- Z - lm(Z~X,weights=w)$fitted#
	reg <- lm(Yres~Zres,weights=w)#
	symbols(Zres,Yres,circles=scale*.1*w/diff(range(Yres)),inches=FALSE,xlab="Residualized Z",ylab="Residualized Y",main=paste("Estimated ATE = ",round(reg$coefficients["Zres"],3)))#
	abline(reg)#
	}
randfun.default <-#
function(desmat.out) {#
#
    desmat <- desmat.out$desmat#
    Z <- desmat.out$Z#
    clustvar <- desmat.out$clustvar#
    blockvar <- desmat.out$blockvar#
#
    perm <- rep(0, length(Z))#
    for(b in 1:nrow(desmat)){#
                block.tr <- sample(desmat[b,]$n,desmat[b,]$m)#
                perm[blockvar==b][block.tr] <- 1#
        }#
    perm.index <- c(1:length(perm))#
    permclus <- perm[match(clustvar, perm.index)]#
    return(permclus)#
}
invert.ci <-#
function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
		#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess > targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 100#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
	#
#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
#
#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
	#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
	#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}
genprobexact <-#
function(    Z,#
                        blockvar = NULL,#
                        clustvar = NULL) {#
#
    desmat.out <- desmat.sanitize(Z,blockvar,clustvar)#
    #
    Z <- desmat.out$Z#
    clustvar <- desmat.out$clustvar#
    blockvar <- desmat.out$blockvar#
#
    probs <- ave(Z, blockvar)#
    probs.index <- c(1:length(probs))#
    probsclus <- probs[match(clustvar, probs.index)]#
    return(probsclus)#
	}
genperms <-#
function(   Z,#
                        blockvar = NULL,#
                        clustvar = NULL,#
                        maxiter=10000) {#
                        	#
	if (is.null(clustvar)) clustvar <- c(1:length(Z))#
#
    desmat.out <- desmat.sanitize(Z,blockvar,clustvar)#
    desmat <- desmat.out$desmat#
    B <- nrow(desmat)#
#
    rands <- prod(desmat$rand)#
    if(rands > maxiter){#
        cat(paste("Too many permutations to use exact method.\nDefaulting to approximate method.\nIncrease maxiter to at least ", rands, " to perform exact estimation.\n",sep=""))#
        permclus <- replicate(maxiter,do.call(randfun.default,list(desmat.out)))#
    }#
#
    if(rands <= maxiter) {#
        perms <- as.matrix(1)#
        #
        unitind <- rep(NA,length(Z))#
        for(b in 1:B){#
                Z.b <- desmat.out$Z[desmat.out$blockvar==b]#
                N.b <- length(Z.b)#
                m.b <- sum(Z.b)#
                perms.b <- combn(N.b, m.b, tabulate, nbins = N.b)#
                perms <- rbind( t(rep(1,ncol(perms.b))%x%t(perms)),#
                                t(t(perms.b)%x%rep(1,ncol(perms))))#
            }#
            #
        perms <- perms[-1,]#
#
        rownames(perms) <- c(1:nrow(perms))#
#
        permclus <- matrix(NA, nrow=length(desmat.out$clustvar), ncol=ncol(perms))#
        permclus <- perms[match(desmat.out$clustvar, rownames(perms)),]#
    }#
    return(permclus)#
}
estlate <-#
function(Y,D,Z,X=NULL,Ypre=NULL,Dpre=NULL,prob=NULL,HT=FALSE) {#
	late <- estate(Y,Z,X,Ypre,prob,HT)/estate(D,Z,X,Dpre,prob,HT)#
	return(late)#
	}
desmat.sanitize <-#
function(  Z,#
                        blockvar = NULL,#
                        clustvar = NULL)#
                        {#
        # To make blockvar sequence of integers from 1 to number of blocks#
        if (!is.null(blockvar)){#
            if(is.character(blockvar)){blockvar <- as.numeric(as.factor(blockvar))}#
            if(is.factor(blockvar)){blockvar <- as.numeric(blockvar)}#
            blockvar <- match(blockvar,unique(blockvar))#
        }#
        if (is.null(blockvar)) blockvar <- rep(1,length(Z))#
        B <- max(blockvar)#
#
        # To make clustvar sequence of integers from 1 to number of clusters#
        if (!is.null(clustvar)){#
            if(is.character(clustvar)){clustvar <- as.numeric(as.factor(clustvar))}#
            if(is.factor(clustvar)){clustvar <- as.numeric(clustvar)}#
            clustvar.sc <- clustvar#
            clus.count <- 1#
            for(i in 1:B){#
                clus.b.i <- unique(clustvar[blockvar==i])#
                for(j in clus.b.i){#
                    clustvar.sc[blockvar==i&clustvar==j] <- clus.count#
                    clus.count <- clus.count+1#
                    }#
                }#
            clustvar <- clustvar.sc#
        }#
        if (is.null(clustvar)) clustvar <- c(1:length(Z))#
#
        Z <- aggregate(Z,list(clustvar),mean)[,2]#
        #ave(Z, clustvar)#
        blockvar <- aggregate(blockvar,list(clustvar),mean)[,2]#
        #ave(blockvar, clustvar)#
#
        desmat <- data.frame(n=NA,m=NA, rand=NA)#
        for(b in 1:B){#
            desmat[b,"n"]    <- length(Z[blockvar==b])#
            desmat[b,"m"]    <- sum(Z[blockvar==b])#
            desmat[b,"rand"] <- choose(length(Z[blockvar==b]),sum(Z[blockvar==b]))#
        }#
    return(list(desmat=desmat,Z=Z,blockvar=blockvar,clustvar=clustvar))#
}
#
rm(list=ls())       # clear objects in memory
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/lucas/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 13")
mail <- read.dta("Chapter 13_Middleton and Rogers (2010) Dataset.dta")#
colnames(mail)
Z  <- as.integer(mail$treatment) - 1          # treatment#
X  <- mail$dem_perf_06        # covariate: Dem voteshare in 2006#
Y  <- mail$relevant_measures_net#
N <- length(Z)
randfun <- function() {#
	teststat <- 100#
	while (teststat > 0.5) {#
		Zri <- sample(Z)#
		teststat <- summary(lm(X~Zri))$coefficients[2,1]   # extract the coefficient#
	}#
	return(Zri)#
}
genperms.custom <-#
function(numiter=10000,randfun=randfun.default,...) {#
	arglist <- list(...)#
	return(replicate(numiter,do.call(randfun,arglist)))#
	}
perms <- genperms.custom(numiter=10000,randfun=randfun)    # notice the use of the restricted randomization function
genprob <-#
function(perms) {#
	prob <- rowMeans(perms)#
	prob0 <- prob==0#
	prob1 <- prob==1#
	if(any(prob0)) {#
		warning(paste("Unit", c(1:length(prob))[prob0], "never enters treatment.\n"))#
	}#
	if(any(prob1)) {#
		warning(paste("Unit", c(1:length(prob))[prob1], "never enters control.\n"))#
	}#
	return(prob)#
	}
probs <- genprob(perms)
estate <-#
function(Y,Z,X=NULL,Ypre=NULL,prob=NULL,HT=FALSE) {#
#
# Raj difference if a pretest is available#
	Yd <- Y#
	if(!is.null(Ypre)) Yd <- Y - Ypre#
	#
	if(is.null(prob)) {#
		warning("Probabilities not specified. Assuming equal probabilities.")#
		prob <- rep(.5,length(Yd))#
	}#
	#
	# does not work?#
#
	weights <- Z/prob + (1-Z)/(1-prob)#
#
# IPW#
	if (HT == FALSE & is.null(X)) {#
		ate <- weighted.mean(Yd[Z==1],weights[Z==1]) - weighted.mean(Yd[Z==0],weights[Z==0])#
#
	}#
#
# IPW + Covariates#
	if (HT == FALSE & !is.null(X)) {#
		ate <- lm(Yd~Z+X,weights = weights)$coefficients[2]#
	}#
	#
# HT#
	if (HT == TRUE & is.null(X)) ate <- (sum(Yd[Z==1]*weights[Z==1]) - sum(Yd[Z==0]*weights[Z==0]))/length(Yd)#
#
# Raj-Within#
	if (HT == TRUE & !is.null(X)) {#
		Ypre2 <- lm(Yd~X)$fitted#
		Yd2 <- Yd - Ypre2#
		ate <- (sum(Yd2[Z==1]*weights[Z==1]) - sum(Yd2[Z==0]*weights[Z==0]))/length(Yd)#
		}#
	#
	return(ate)#
	}
ate <- estate(Y,Z,prob=probs)
genouts <-#
function(Y,Z,ate=0) {#
	Y0 <- Y1 <- Y#
	Y0[Z==1] <- Y[Z==1] - ate#
	Y1[Z==0] <- Y[Z==0] + ate#
	return(Ys=list(Y0=Y0,Y1=Y1))#
	}
Ys <- genouts(Y,Z,ate=0)
gendist <-#
function(Ys,perms,X=NULL,Ypre=NULL,prob=NULL,HT=FALSE) {#
#
	numiter <- ncol(perms)#
	distout <- rep(NA,numiter)#
	#
#	if (is.null(Ds)) Ds <- list(D1=rep(1,length(Ys)),D0=rep(0,length(Ys)))#
#
	# switch to apply?#
#
	for (iter in 1:numiter) {#
		Zri <- perms[,iter]#
		Yri <- Ys$Y0#
		Yri[Zri==1] <- Ys$Y1[Zri==1]#
#
		distout[iter] <- estate(Yri,Zri,X,Ypre,prob,HT)#
		}		#
		#
	return(distout)#
	}
#
distout <- gendist(Ys,perms,prob=probs)
ate
dispdist <-#
function(distout,ate,quantiles=c(0.025,0.975),display.plot=TRUE) {#
	#
	Pse <- function(x) mean((x - mean(x))^2)^.5#
	#
	greater.p.value <- mean(distout >= ate)#
	lesser.p.value <- mean(distout <= ate)#
	p.value <- 2*min(greater.p.value,lesser.p.value)#
	#mean(abs(distout) >= abs(ate)) # not right?#
	#
	conf.int <- quantile(distout,quantiles)#
	se <- Pse(distout)#
	#
	if (display.plot == TRUE) hist(distout,freq=TRUE,xlab="Estimated ATE",main=paste("Distribution of the Estimated ATE"),breaks=length(distout)^.5,lwd=1)#
		#
	return(list(two.tailed.p.value=p.value,greater.p.value=greater.p.value,lesser.p.value=lesser.p.value,conf.int=conf.int,se=se))#
	}
dispdist(distout,ate)
summary(lm(X~Zri))
Zri <- sample(Z)
summary(lm(X~Zri))$coefficients
#
rbind(cbind("mean","SD"),cbind(mean(Y[Z==0]),sd(Y[Z==0])),cbind(mean(Y[Z==1]),sd(Y[Z==1])))
Term <- read.dta("Chapter 13_Titiunik (2010) Dataset.dta")#
#
attach(Term)#
#
Term2003 <- subset(Term, year==2003)#
attach(Term2003)#
as.data.frame(Term2003)#
write.csv(Term2003,"Titiunik2010ch13year2003.csv")#
Term2003 <- read.csv(file="Titiunik2010ch13year2003.csv", header=TRUE)#
#
Z_alpha <- dshort_term#
#
Z <- as.numeric(dshort_term=="4 years")#
#
#Let's rename three variables#
X1 <-  party   #
X2 <-  usrepvotesh_dem #
X3 <-  votesh_incumb#
#
Y <- bills_intro#
#
# calculate apparent ATE#
#
mean(Y[Z==1])-mean(Y[Z==0])#
#
# draw individual values plot but with axes switched #
## this is 13.1 graph#
set.seed(123456)#
plot(Y~jitter(Z,factor=0.5),main="Distributions under Treatment and Control",ylab="Number of Bills Introduced by State Senator",yaxp=c(0,150,5),ylim=c(0,150),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("2-Year Term", "4-Year Term"), at=c(0,1))
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 13/")#
#
#  Data are from Middleton, Joel, and Todd Rogers. 2010. Defend Oregons Voter Guide Program. Report for the Analyst Institute. #
#
mail <- read.dta("Chapter 13_Middleton and Rogers (2010) Dataset.dta")#
colnames(mail)#
#
Z  <- as.integer(mail$treatment) - 1          # treatment#
X  <- mail$dem_perf_06        # covariate: Dem voteshare in 2006#
Y  <- mail$relevant_measures_net#
N <- length(Z)#
#
# Part (a & b): compare means by treatment group#
#
rbind(cbind("mean","SD"),cbind(mean(Y[Z==0]),sd(Y[Z==0])),cbind(mean(Y[Z==1]),sd(Y[Z==1])))
#
plot(Y~jitter(Z,factor=0.5),main="Distributions under Treatment and Control",ylab="Number of Bills Introduced by State Senator",yaxp=c(0,150,5),ylim=c(0,150),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Treatement", "Control"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Distributions under Treatment and Control",ylab="Number of Bills Introduced by State Senator",yaxp=c(-200,200,5),ylim=c(-200,200),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Treatement", "Control"), at=c(0,1))
mail
summary(mail)
summary(mail[Z==1])
summary(mail[Z==1,])
summary(mail[Z==0,])
plot(Y~jitter(Z,factor=0.5),main="Distributions under Treatment and Control",ylab="Number of Bills Introduced by State Senator",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Treatement", "Control"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Distributions under Treatment and Control",ylab="Number of Bills Introduced by State Senator",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
#
#
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
#
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Control","Treatement"), at=c(0,1), cex=2)
help(par)
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
#
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n")#
axis(1, labels=c("Control","Treatement"), at=c(0,1), cex=3)
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=3)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=c(1,2))#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=X)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=X/20)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=X/25)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition \n test",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=X/25)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition \n (Size proportional to Democratic Vote Share)",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=X/25)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
#
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition \n (Size Proportional to Democratic Vote Share)",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=X/25)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
# Part (c): RI under complete randomization#
#
probs <- genprobexact(Z)          # generate probability of treatment #
#
ate <- estate(Y,Z,prob=probs)     # estimate the ATE#
#
perms <- genperms(Z,maxiter=10000)  # set the number of simulated random assignments#
#
Ys <- genouts(Y,Z,ate=0)            # create potential outcomes under the sharp null of no effect for any unit#
#
distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
#
ate                                 # estimated ATE#
dispdist(distout,ate)               # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null
abline(v=ate, col=rgb(255,0,0,70,maxColorValue=255))
abline(v=ate, col=rgb(255,0,0,70,maxColorValue=255), lwd=2)
# part (d)#
# set up a restricted randomization whereby random allocations (Zri) are discarded if they generate an absolute coefficient smaller than 0.5 when the covariate X is regressed on Zri (which is the same as saying that the treatment and control means in terms of X are closer than 0.5 percentage points)#
#
randfun <- function() {#
	teststat <- 100             # initialize teststat at an inadmissible value#
	while (teststat > 0.5) {#
		Zri <- sample(Z)        # randomly allocate#
		teststat <- summary(lm(X~Zri))$coefficients[2,1]   # extract the coefficient representing the difference-in-means and reject if it violates the while condition#
	}#
	return(Zri)#
}
perms <- genperms.custom(numiter=10000,randfun=randfun)    # notice the use of the restricted randomization function in the generation of simulated random allocations#
#
probs <- genprob(perms)           # important: restricted randomization can sometimes generate unequal probabilities of assignment, so it's important to generate the probs and use inverse probability weights when estimating the ATE#
#
ate <- estate(Y,Z,prob=probs)    #
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
#
dispdist(distout,ate)
Zri
##LL#
plot(Y~jitter(Z,factor=0.5),main="Outcomes for Treatment and Control",ylab="Number of Net Votes Won by Sponsor",yaxp=c(-600,900,5),ylim=c(-600,800),xlab="Treatment Condition \n (Size Proportional to Democratic Vote Share)",xaxp=c(0,1,1),xlim=c(-0.5,1.5), xaxt="n", cex=X/25)#
axis(1, labels=c("Control","Treatement"), at=c(0,1))
