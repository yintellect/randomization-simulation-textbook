program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages/2),replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
length[storemean]
length(storemean)
lower
upper
length(villages)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages/2),replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
length(villages)
mean(villages)
cbind(rep(0,25),rep(1,75))
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- rbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
rbind(rep(0,25),rep(1,75))
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
villages
length(villages)
lower
upper
length(storemean)
mean(random_draw)
??"decimals"
?ndec
??"digits"
?digit
??digit
??digits
?display
??display
??environment
sys.getenv
??environment
help base
?base
library(help="base")
Sys.localeconv
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result*100#
upper_result*100
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result*10#
upper_result*10
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(random_draw)
mean(control_draw)
length(villages)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages/2)))#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
length(ethnicity)
corr(ethnicity,treatment)
r(ethnicity,treatment)
correlation(ethnicity,treatment)
?correlation
??correlation
mean(treatment)
length(treatment)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(ethnicity,treatment)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
corr(ethnicity,treatment)
??correlation
corr(ethnicity,treatment)
length(ethnicity)
length(treatment)
?corr
corr(cbind(ethnicity,treatment))
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr)
mean(ethnicity)
mean(treatment)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr)
cbind(ethnicity,treatment)
mean(ethnicity)
control_draw
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
seed <- 1337#
rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
3+3
?sort
??sort
random_draw
table(random_draw)
nrows(table(random_draw))
nrow(table(random_draw))
mean(random_draw)
control_draw <- c(rep(1,75-table(random_draw)[2,2]))
test=table(random_draw)
test[2,2]
test[2,1]
test(2,1)
length(test)
test[1]
test[1,1]
test
test[2]
length(test[2])
length(test[1])
nrow(test)
ncol(test)
test[1]*test[2]
test2 <- cbind(rand_sort,villages)
test2
sort(test2)
as.matrix(test)
testmat <- as.matrix(test)
testmat
testmat[1,1]
dims(testmat)
dim(testmat)
test
testmat
nrow(testmat)
ncol(testmat)
testmat[1,1]
testmat[2,1]
ethnicity[treatment ==1]
ethnicity[treatment ==0]
random_draw
mean(random_draw)
table(random_draw)
mean(villages)
mean(villages)*length(villages)
mean(random_draw)*length(villages)/2
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones)#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))Ã¥#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
control_ones
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^0.5#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr2)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 1000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^0.5#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
storecorr
storecorr2
ethnicity
mean(random_draw)
mean(control_draw)
length(random_draw)
length(control_draw)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 1000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 1000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result#
#
mean(storecorr2)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result#
#
mean(storecorr2)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
# number of villages in simulation#
length(villages)#
lower_result#
upper_result#
#
mean(storecorr2)
hist(storecorr2)
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
# number of villages in simulation#
length(villages)#
lower_result#
upper_result#
#
storecorr2 <- sort(storecorr2)#
mean(storecorr2)
storecorr2[2500]
storecorr2[97500]
storecorr2[99990]
storecorr2[99999]
program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
# number of replications#
numiter#
# number of villages in simulation#
length(villages)#
lower_result#
upper_result#
#
storecorr2 <- sort(storecorr2)#
mean(storecorr2)
storecorr2[97500]
storecorr2[99900]
hist(storecorr2)
?logit
??logit
?logit
?"logistic regression"
??"logistic regression"
??"robust cluster"
??"cluster"
Z <- dshort_term=="4 years"
ibrary(foreign)#
Term <- read.dta("Chapter 13_Titiunik (2010) Dataset.dta")#
#
attach(Term)#
#
Z_alpha <- dshort_term
set.seed(1234567)#
#
library(ri)#
library(foreign)#
#
hough <- read.dta("/Users/donaldgreen/Dropbox/Field Experimentation Book/Datasets for Website/Chapter 8_Leslie Hough self-experiment data.dta")#
#
# Part (b)#
#
Y <- hough$tetris#
Z <- hough$run#
#
N <- length(Z)#
#
Zlag <- c(NA,Z[2:N-1]) # exclude day 1 from analysis#
Ylag <- c(NA,Y[2:N-1])#
#
randfun <- function() rbinom(N,1,.5)#
#
numiter <- 10000#
perms <- genperms.custom(numiter=numiter,randfun=randfun)#
#
test1 <- lm(Y~Z)$coefficients["Z"]#
test2 <- summary(lm(Y~Z+Zlag))$fstatistic[1]#
test3 <- lm(Ylag~Z)$coefficients["Z"]#
test4 <- lm(hough$energy~Z)$coefficients["Z"]#
test5 <- lm(hough$gre~Z)$coefficients["Z"]#
#
testdist1 <- testdist2 <- testdist3 <- testdist4 <- testdist5 <- rep(NA,numiter)#
#
for (i in 1:numiter) {#
	Zri <- perms[,i]#
	Zlagri <- c(NA,Zri[2:N-1]) # exclude day 1 from analysis#
#
testdist1[i] <- lm(Y~Zri)$coefficients["Zri"]#
testdist2[i] <- summary(lm(Y~Zri+Zlagri))$fstatistic[1]#
testdist3[i] <- lm(Ylag~Zri)$coefficients["Zri"]#
testdist4[i] <- lm(hough$energy~Zri)$coefficients["Zri"]#
testdist5[i] <- lm(hough$gre~Zri)$coefficients["Zri"]#
	}#
mean(testdist1 >= test1)#
mean(testdist2 >= test2)#
mean(abs(testdist3) >= abs(test3))#
mean(testdist4 >= test4)#
mean(testdist5 >= test5)
library(ri)#
#
set.seed(1)#
#
Y1 <- c(5,15,12,19,17,18,24,11,16,25,18,21,17,24,27,26,30,37,43,39,36,27,33,37,48,39,42,37,53,50,51,43,55,49,48,52,59,52,55,63)#
Y0 <- c(5,5,6,9,10,11,12,13,14,19,20,20,20,21,24,25,27,27,30,32,32,32,32,35,35,37,38,38,41,42,43,44,45,47,48,51,52,52,57,62)#
X <- c(6,8,5,13,9,15,16,17,19,23,28,28,9,16,23,15,23,33,42,31,29,28,35,28,41,37,32,37,36,44,48,43,55,53,51,43,57,51,49,55)#
#
mean(Y1-Y0)#
#
### DGP.#
#
Z <- c(0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,0,1,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,0,1,1)#
Y <- Y0*(1-Z) + Y1*(Z)#
N <- length(Z)#
#
# Part (a)#
#
lm(Y~Z)#
mean(Y[Z==1])-mean(Y[Z==0])#
#
# Part (b)#
#
lm(Y~X,subset=Z==1)#
lm(Y~X,subset=Z==0)#
#
# Part (c)#
#
lm(Y~Z+X)#
#
# Part (d)#
#
perms <- genperms(Z,maxiter=100000)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
#
dispdist(distout,ate)#
#
# Part (e)#
#
ateX <- estate(Y,Z,X,prob=probs)#
#
distoutX <- gendist(Ys,perms,X,prob=probs)#
#
ateX#
#
dispdist(distoutX,ateX)#
#
# Part (f)#
#
Ys2 <- genouts(Y,Z,ate=ate)#
#
distout2 <- gendist(Ys2,perms,prob=probs)#
#
ate#
#
dispdist(distout2,ate)#
#
# Part (g)#
#
YsX <- genouts(Y,Z,ate=ateX)#
#
distoutX2 <- gendist(YsX,perms,X,prob=probs)#
#
ateX#
#
dispdist(distoutX2,ateX)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 5#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 50#
ncontrol <- 350#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
ate
mean(Y0)
mean(Y1)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 5#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 5#
ncontrol <- 15#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 10#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 5#
ncontrol <- 15#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 100#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 5#
ncontrol <- 35#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 100#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 5#
ncontrol <- 10#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 100#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 10#
ncontrol <- 20#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
dim(Pupperci)
length(Pupperci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 500#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 10#
ncontrol <- 20#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 200#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 10#
ncontrol <- 20#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 5#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 200#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 20#
ncontrol <- 10#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 5#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 200#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 20#
ncontrol <- 10#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 200#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 20#
ncontrol <- 10#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 1.2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 200#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 20#
ncontrol <- 10#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 1#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 200#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 20#
ncontrol <- 20#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 1#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
#
# Small bugfix for invert.ci code -- will make it into next version of the package#
#
invert.ci <- function(Y,Z,prob,perms,targetp) {#
#
	ate <- estate(Y,Z,prob=prob)#
	Ys <- genouts(Y,Z,ate)#
	distro <- gendist(Ys,perms,prob=prob)#
	mindistro <- quantile(distro,mean(c(targetp,0)))#
	maxdistro <- quantile(distro,mean(c(targetp,1)))#
	ATEg <- ATEgorig <- quantile(distro,targetp)#
	bw <- min(abs(mindistro-ATEg),abs(maxdistro-ATEg))#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	pguess <- mean(dist1 >= testS)#
#
if (pguess >= targetp) bound <- ATEg - bw#
if (pguess < targetp) bound <- ATEg + bw#
# see if bound is good enough; might need to go farther#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
#
counter.max <- 200#
counter <- 0#
#
while (pguess > targetp & pguessM > targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg - bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)	#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
	}#
while (pguess < targetp & pguessM < targetp) {#
	temp <- ATEg#
	ATEg <- bound#
	bound <- ATEg + bw#
#
	YsM <- genouts(Y-bound*Z,Z,0)#
	testM <- estate(Y-bound*Z,Z,prob=prob)#
	distM <- gendist(YsM,perms,prob=prob)#
	pguessM <- mean(distM >= testM)#
	counter <- counter + 1#
	if (counter >= counter.max) stop("Cannot Reach p.")#
}#
findroot <- function(ATEg,targetp) {#
	Ys1 <- genouts(Y-ATEg*Z,Z,0)#
	testS <- estate(Y-ATEg*Z,Z,prob=prob)#
	dist1 <- gendist(Ys1,perms,prob=prob)#
	return(mean(dist1 >= testS) - targetp)#
	}#
if (pguessM == targetp) {#
	ATEg <- bound#
	pguess <- targetp#
	}#
if (pguess != targetp) {#
	lowint <- uniroot(findroot,c(bound,ATEg),targetp=targetp)#
	lowintM <- lowint$root#
} else lowintM <- ATEg#
#
return(lowintM)#
}#
#
##### Code begins -- parameters#
#
ntreat <- 5#
ncontrol <- 10#
N <- ntreat+ncontrol#
#
# ratio of treatment to control variance#
ratio <- 2#
#
Y0 <- rnorm(N)#
Y0 <- Y0 - mean(Y0)#
Y1 <- rnorm(N)*ratio#
Y1 <- Y1 - mean(Y1)#
#
# True ATE is Zero.#
#
Z <- c(rep(1,ntreat),rep(0,ncontrol))#
#
perms <- genperms(Z,maxiter=200)#
#
upperci <- lowerci <- rep(NA,ncol(perms))#
Nupperci <- Nlowerci <- rep(NA,ncol(perms))#
Pupperci <- Plowerci <- rep(NA,ncol(perms))#
#
for (i in 1:ncol(perms)) {#
	Zri <- perms[,i]#
	Y <- Y0*(1-Zri) + Y1*Zri#
	upperci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.975)#
	lowerci[i] <- invert.ci(Y,Zri,prob=rep(mean(Zri),N),perms=perms,targetp=0.025)#
	seest <- (var(Y[Zri==1])/sum(Zri) + var(Y[Zri==0])/sum(1-Zri))^.5#
	ateest <- mean(Y[Zri==1]) - mean(Y[Zri==0])#
	Nupperci[i] <- ateest + 1.96*seest#
	Nlowerci[i] <- ateest - 1.96*seest#
	Ys <- genouts(Y,Zri,ate=ateest)#
	cis <- quantile(gendist(Ys,perms,prob=rep(mean(Zri),N)),c(0.025,0.975))#
	Plowerci[i] <- cis[1]#
	Pupperci[i] <- cis[2]#
	cat(i,"")#
}#
#
# Coverage of Neyman + normal approx#
mean(Nupperci >= 0 & Nlowerci <= 0,na.rm=TRUE)#
# Coverage of Rosenbaum method#
mean(upperci >= 0 & lowerci <= 0,na.rm=TRUE)#
# Coverage of Aronow method#
mean(Pupperci >= 0 & Plowerci <= 0,na.rm=TRUE)#
#
# Summarizing intervals#
summary(Nupperci)#
summary(upperci)#
summary(Pupperci)#
#
summary(Nlowerci)#
summary(lowerci)#
summary(Plowerci)
library(ri)#
set.seed(1234567)#
Y0 <- c(0,1,2,4,4,6,6,9,14,15,16,16,17,18)#
Y1 <- c(0,0,1,2,0,0,2,3,12,9,8,15,5,17)#
#
Z <- c(1,1,0,0,0,0,0,0,0,0,0,0,1,1)#
#
compperms <- genperms(Z)#
numperms <- ncol(compperms)#
#
compmeans <- rep(NA,numperms)#
#
for (i in 1:numperms) compmeans[i] <- mean(Y1[compperms[,i]==1]) - mean(Y0[compperms[,i]==0])#
#
block <- c(1,1,1,1,1,1,1,1,2,2,2,2,2,2)#
#
blockperms <- genperms(Z,block)#
numperms <- ncol(blockperms)#
#
blockmeans <- rep(NA,numperms)#
#
for (i in 1:numperms) blockmeans[i] <- weighted.mean(Y1[blockperms[,i]==1],c(8/2,8/2,6/2,6/2)) - weighted.mean(Y0[blockperms[,i]==0],c(8/6,8/6,8/6,8/6,8/6,8/6,6/4,6/4,6/4,6/4))#
#
save(compmeans,blockmeans,file="figure3.1.Rdata")#
#
par(mfrow=c(2,1))#
hist(compmeans,main="Sampling Distribution under Complete Randomization",xlim=c(-15,10),xlab="ATE Estimates",freq=FALSE,ylim=c(0,.3))#
hist(blockmeans,main="Sampling Distribution under Blocked Randomization",xlim=c(-15,10),xlab="ATE Estimates",freq=FALSE,ylim=c(0,.3))#
#
# calculate the proportion of esitmates that are above zero#
#
length(compmeans[compmeans > 0])#
length(compmeans[compmeans > 0])/length(compmeans)#
#
length(blockmeans[blockmeans > 0])#
length(blockmeans[blockmeans > 0])/length(blockmeans)
rm(list = ls())#
#
set.seed(1337)#
#
NS <- 30#
ntreatedS <- 10#
radius <- .5#
radiusW <- .25#
radiusB <- .75#
#
numrands <- 10000#
#
ax <- rnorm(NS*2)#
#
coordsS <- cbind(ax[1:(NS)],ax[(NS+1):(2*NS)])#
#
numrepeater <- 1#
#
N <- NS*numrepeater#
ntreated <- ntreatedS*numrepeater#
coords <- coordsS[rep(c(1:NS),numrepeater),] + cbind(2*(max(coordsS[,1])-min(coordsS[,1])+radius)*sort(rep(c(0:(numrepeater-1)),NS)),0)#
#
NnonexpS <- 100#
coordsNonS <- cbind(rnorm(NnonexpS),rnorm(NnonexpS))#
#
Nnonexp <- NnonexpS*numrepeater#
#
coordsNon <- coordsNonS[rep(c(1:NnonexpS),numrepeater),] + cbind(2*(max(coordsS[,1])-min(coordsS[,1])+radius)*sort(rep(c(0:(numrepeater-1)),NnonexpS)),0)
distmat <- as.matrix(dist(coords))#
#
# non-experimental distances to experimental units#
distmatNon <- as.matrix(dist(rbind(coords,coordsNon)))[1:N,(N+1):(N+Nnonexp)]#
#
numnear <- apply(distmat,1,function(x) sum(x < radius)) - 1#
numnearW <- apply(distmat,1,function(x) sum(x < radiusW)) - 1#
numnearB <- apply(distmat,1,function(x) sum(x < radiusB)) - 1#
#
numnearNon <- apply(distmatNon,2,function(x) sum(x < radius))#
numnearNonW <- apply(distmatNon,2,function(x) sum(x < radiusW))#
numnearNonB <- apply(distmatNon,2,function(x) sum(x < radiusB))#
#
Y00 <- 10 + numnearB*10#
#
t01 <- -5#
t10 <- 5#
t11 <- -7#
#
Y01 <- Y00 + t01#
Y10 <- Y00 + t10#
Y11 <- Y00 + t11#
#
treat <- sample(c(rep(0,N-ntreated),rep(1,ntreated)))#
#sample(sample(sample(c(rep(1,ntreated),rep(0,N-ntreated)))))#
#
mean(Y00[numnear>0])#
mean(Y10[numnear>0])#
mean(Y01[numnear>0])#
mean(Y11[numnear>0])
mean(Y00)
Y00Non <- 0 + numnearNonB#
Y10Non <- Y00Non + t10#
#
distmattreat <- distmat + (1-treat)*100#
numtreat <- apply(distmattreat,2,function(x) sum(x < radius)) - treat#
cond <- 10*(numtreat > 0) + treat#
#
numtreatW <- apply(distmattreat,2,function(x) sum(x < radiusW)) - treat#
condW <- 10*(numtreatW > 0) + treat#
#
distmatNontreat <- distmatNon + (1-treat)*100#
numtreatNon <- apply(distmatNontreat,2,function(x) sum(x < radius))#
condNon <- 10*(numtreatNon > 0)#
#
cond[numnear>0]#
table(numnear,treat)#
#
Y <- Y00#
Y[cond==1] <- Y01[cond==1]#
Y[cond==11] <- Y11[cond==11]#
Y[cond==10] <- Y10[cond==10]#
#
mean(Y[treat==1]) - mean(Y[treat==0])#
#
#rands <- combn(N,ntreated)#
#
# 00, 01, 10, 11#
pmat <- pmatW <- matrix(0,N,4)#
#
# 10#
pNon <- rep(0,Nnonexp)#
#
for (i in 1:numrands) {#
	treatri <- sample(treat)#
	distmattreat <- distmat + (1-treatri)*100#
	numtreat <- apply(distmattreat,2,function(x) sum(x < radius)) - treatri#
	condri <- 2*(numtreat > 0) + treatri + 1#
	for (j in 1:N) pmat[j,condri[j]] <- pmat[j,condri[j]] + 1#
	# wrong radius#
	numtreat <- apply(distmattreat,2,function(x) sum(x < radiusW)) - treatri#
	condri <- 2*(numtreat > 0) + treatri + 1#
	for (j in 1:N) pmatW[j,condri[j]] <- pmatW[j,condri[j]] + 1#
	# non-experimental units#
	distmatNontreat <- distmatNon + (1-treatri)*100#
	numtreatNon <- apply(distmatNontreat,2,function(x) sum(x < radius))#
	pNon <- pNon +  (numtreatNon > 0)#
	if (i %% 1000 == 0) cat(i,"")#
	}
summary(lm(Y[numnear == 0]~treat[numnear==0]))
Someone near#
#
pscoremat <- pmat/numrands#
pscorematW <- pmatW/numrands#
pscoreNon <- pNon/numrands#
#
#weight <- rep(NA,0)#
#
weight <- 1/pscoremat[,1]#
#
for (j in 1:N) {#
	if (cond[j] == 01) weight[j] <- 1/pscoremat[j,2]#
	if (cond[j] == 10) weight[j] <- 1/pscoremat[j,3]#
	if (cond[j] == 11) weight[j] <- 1/pscoremat[j,4]#
	}#
#
######### Get Distribution#
#
Y00mean <- Y10mean <- Y01mean <- Y11mean <- Y0meanN <- Y1meanN <- rep(NA,numrands)#
#
Y00meanW <- Y10meanW <- Y01meanW <- Y11meanW <- Y0meanNW <- Y1meanNW <- rep(NA,numrands)#
Y00meanna <- Y10meanna <- Y01meanna <- Y11meanna <- rep(NA,numrands)#
Y00meanT <- Y10meanT <- Y01meanT <- Y11meanT <- Y0meanNT <- Y1meanNT <- rep(NA,numrands)#
tauS <- tauNon <- rep(NA,numrands)
Nright <- sum(numnear >0)#
#
for (i in 1:numrands) {#
	treatri <- sample(treat)#
	distmattreat <- distmat + (1-treatri)*100#
	numtreat <- apply(distmattreat,2,function(x) sum(x < radius)) - treatri#
	condri <- 10*(numtreat > 0) + treatri#
	Yri <- Y00#
	Yri[condri==1] <- Y01[condri==1]#
	Yri[condri==11] <- Y11[condri==11]#
	Yri[condri==10] <- Y10[condri==10]#
#
	Y1meanN[i] <- mean(Yri[numnear == 0 & treatri == 1])#
	Y0meanN[i] <- mean(Yri[numnear == 0 & treatri == 0])#
#
	weightri <- 1/pscoremat[,1]#
#
	for (j in 1:N) {#
		if (condri[j] == 01) weightri[j] <- 1/pscoremat[j,2]#
		if (condri[j] == 10) weightri[j] <- 1/pscoremat[j,3]#
		if (condri[j] == 11) weightri[j] <- 1/pscoremat[j,4]#
	}#
	Y00meanT[i] <- sum(Yri[numnear > 0 & condri == 00]*weightri[numnear > 0 & condri == 00])/Nright#
	Y00mean[i] <- weighted.mean(Yri[numnear > 0 & condri == 00],weightri[numnear > 0 & condri == 00])#
	Y10meanT[i] <- sum(Yri[numnear > 0 & condri == 10]*weightri[numnear > 0 & condri == 10])/Nright#
	Y10mean[i] <- weighted.mean(Yri[numnear > 0 & condri == 10],weightri[numnear > 0 & condri == 10])#
	Y01meanT[i] <- sum(Yri[numnear > 0 & condri == 01]*weightri[numnear > 0 & condri == 01])/Nright#
	Y01mean[i] <- weighted.mean(Yri[numnear > 0 & condri == 01],weightri[numnear > 0 & condri == 01])#
	Y11meanT[i] <- sum(Yri[numnear > 0 & condri == 11]*weightri[numnear > 0 & condri == 11])/Nright#
	Y11mean[i] <- weighted.mean(Yri[numnear > 0 & condri == 11],weightri[numnear > 0 & condri == 11])#
#
	Y00meanna[i] <- mean(Yri[condri==00])#
	Y01meanna[i] <- mean(Yri[condri==01])#
	Y10meanna[i] <- mean(Yri[condri==10])#
	Y11meanna[i] <- mean(Yri[condri==11])#
#
	tauS[i] <- mean(Yri[treatri==1]) - mean(Yri[treatri==0])
numtreatW <- apply(distmattreat,2,function(x) sum(x < radiusW)) - treatri#
	condriW <- 10*(numtreatW > 0) + treatri#
#
	weightriW <- 1/pscorematW[,1]#
#
	for (j in 1:N) {#
		if (condriW[j] == 01) weightriW[j] <- 1/pscorematW[j,2]#
		if (condriW[j] == 10) weightriW[j] <- 1/pscorematW[j,3]#
		if (condriW[j] == 11) weightriW[j] <- 1/pscorematW[j,4]#
	}#
	Y1meanNW[i] <- mean(Yri[numnearW == 0 & treatri == 1])#
	Y0meanNW[i] <- mean(Yri[numnearW == 0 & treatri == 0])#
	Y00meanW[i] <- weighted.mean(Yri[numnearW > 0 & condriW == 00],weightriW[numnearW > 0 & condriW == 00])#
	Y10meanW[i] <- weighted.mean(Yri[numnearW > 0 & condriW == 10],weightriW[numnearW > 0 & condriW == 10])#
	Y01meanW[i] <- weighted.mean(Yri[numnearW > 0 & condriW == 01],weightriW[numnearW > 0 & condriW == 01])#
	Y11meanW[i] <- weighted.mean(Yri[numnearW > 0 & condriW == 11],weightriW[numnearW > 0 & condriW == 11])
non-experimental#
#
	distmatNontreat <- distmatNon + (1-treatri)*100#
	numtreatNon <- apply(distmatNontreat,2,function(x) sum(x < radius))#
	condriNon <- 10*(numtreatNon > 0)#
#
	tauNon[i] <- weighted.mean(Y10Non[numnearNon > 0 & condriNon == 10],1/pscoreNon[numnearNon > 0 & condriNon == 10]) - weighted.mean(Y00Non[numnearNon > 0 & condriNon == 00],1/(1-pscoreNon)[numnearNon > 0 & condriNon == 00])#
#
	if (i %% 1000 == 0) cat(i,"")#
	}#
#
######### True#
#
mean(Y00[numnear>0])#
summary(Y00mean)#
summary(Y00meanT)#
#
mean(Y10[numnear>0])#
summary(Y10mean)#
summary(Y10meanT)#
#
mean(Y01[numnear>0])#
summary(Y01mean)#
summary(Y01meanT)#
#
mean(Y11[numnear>0])#
summary(Y11mean)#
summary(Y11meanT)
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
r[y<0] <- 0#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
r
r[y0<0 | y1<0] <- 0
mean(r)
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
#r[y<0] <- 0#
r[y0<0 | y1<0] <- 0#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
r[y<0] <- 0#
#r[y0<0 | y1<0] <- 0#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
r[y<0] <- 0   # Cyrus's version of missingness based on realized outcomes#
#r[y0<0 | y1<0] <- 0   # Don's version of missingness based on potential outcomes#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
#r[y<0] <- 0   # Cyrus's version of missingness based on realized outcomes#
r[y0<0 | y1<0] <- 0   # Don's version of missingness based on potential outcomes#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
dva
tr
trx
?I
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
#r[y<0] <- 0   # Cyrus's version of missingness based on realized outcomes#
r[y0<0 | y1<0] <- 0   # Don's version of missingness based on potential outcomes#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
?par
I(r)
I(1-r)
test <- 1 1 1 1
test <- c(1,1,1,1)
I(test)
test <- c(1,1,1,0)
I(test)
test <- c(1,1,2,0)
I(test)
1-r
trx
dva
rm(list=ls())#
#
n <- 1000#
x <- rnorm(n)#
#
y0 <- x + rnorm(n)#
y1 <- x+1.25  + rnorm(n)#
#
# x response indicator#
r0 <- as.numeric(y0>=0) #
r1 <- as.numeric(y1>=0) #
#
# Imputing 0 for missing x#
x0.m <- r0*x#
x1.m <- r1*x#
#
# estimation data#
y.all <- c(y0,y1)#
d <- c(rep(0,length(y0)),rep(1,length(y1)))#
x.all <- rep(x,2)#
x.m.all <- c(x0.m,x1.m)#
r.all <- c(r0,r1)#
#
# Fit with complete data#
b.true <- coef(lm(y.all~d+x.all))#
#
# Dummy variable adjustment fit #
b.dva <- coef(lm(y.all~d+x.m.all+r.all))
b.true
b.dva
r.all
rm(list=ls())#
#
n <- 1000#
x <- rnorm(n)#
#
y0 <- x + rnorm(n)#
y1 <- x+1.25  + rnorm(n)#
#
# x response indicator#
r0 <- as.numeric(y0>=0) #
r1 <- as.numeric(y1>=0) #
#
# Imputing 0 for missing x#
x0.m <- r0*x#
x1.m <- r1*x#
#
# estimation data#
y.all <- c(y0,y1)#
d <- c(rep(0,length(y0)),rep(1,length(y1)))#
x.all <- rep(x,2)#
x.m.all <- c(x0.m,x1.m)#
r.all <- c(r0,r1)#
#
# Fit with complete data#
b.true <- coef(lm(y.all~d+x.all))#
#
# Dummy variable adjustment fit #
b.dva <- coef(lm(y.all~d+x.m.all+r.all))#
#
# Graphical demonstration#
par(mfrow=c(1,3))#
plot(rep(x,2),c(y0,y1),type="n", main=c("Full data (y0 blue, y1 red)",paste("b.hat=",round(b.true[2], digits=2),sep="")),xlab="x",ylab="y")#
points(x,y1, col="red", pch=19)#
points(x,y0, col="blue", pch=19)#
abline(b.true[1],b.true[3],col="gray")#
abline(b.true[1]+b.true[2],b.true[3],col="gray")#
#
plot(rep(x,2),c(y0,y1),type="n", main="Missing if y<0",xlab="x",ylab="y")#
points(x,y1, col="red", pch=19)#
points(x,y0, col="blue", pch=19)#
points(x[y1<0], y1[y1<0], pch="X", col="gray", cex=1.5)#
points(x[y0<0], y0[y0<0], pch="X", col="gray", cex=1.5)#
#
plot(rep(x,2),c(y0,y1),type="n", main=c("Imputation-completed data",paste("b.hat=",round(b.dva[2], digits=2),sep="")),xlab="x",ylab="y")#
points(x0.m,y0, col="blue", pch=19)#
points(x1.m, y1, col="red", pch=19)#
abline(b.dva[1]+b.dva[4],b.dva[3],col="blue",lty="dashed")#
abline(b.dva[1]+b.dva[2]+b.dva[4],b.dva[3],col="red",lty="dashed")#
abline(b.true[1],b.true[3],col="gray")#
abline(b.true[1]+b.true[2],b.true[3],col="gray")
mean(t)
t
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
#r[y<0] <- 0   # Cyrus's version of missingness based on realized outcomes#
r[y0<0 | y1<0] <- 0   # Don's version of missingness based on potential outcomes#
#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
mean(d)
mean(t)
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
#r[y<0] <- 0   # Cyrus's version of missingness based on realized outcomes#
# r[y0<0 | y1<0] <- 0   # Don's version of missingness based on potential outcomes#
r[y0*(t+.3)<0 | y1<0] <- 0   # asymmetrical missingness based on potential outcomes#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
lm(r ~ t)
rm(list=ls())#
#
n <- 5000#
x <- rnorm(n)#
y0 <- x + rnorm(n, sd=.5)#
y1 <- x + 1.25 + rnorm(n, sd=.5)#
t <- rep(0,n)#
t[sample(1:n,n/2)] <- 1#
y <- t*y1 + (1-t)*y0#
par(mfrow=c(1,1))#
plot(x,y, type="n")#
points(x[t==1],y[t==1], col="red")#
points(x[t==0],y[t==0], col="blue")#
#
# Missingness#
r <- rep(1,n)#
#r[y<0] <- 0   # Cyrus's version of missingness based on realized outcomes#
# r[y0<0 | y1<0] <- 0   # Don's version of missingness based on potential outcomes#
r[y0*t<0 | y1<0] <- 0   # asymmetrical missingness based on potential outcomes#
points(x[r==0],y[r==0], col="gray", lwd=1.5)#
#
x2 <- x#
x2[r==0] <- 0#
#
points(jitter(x2[r==0&t==0]),y[r==0&t==0], col="red", pch=19, cex=.5)#
points(jitter(x2[r==0&t==1]),y[r==0&t==1], col="blue", pch=19, cex=.5)#
#
tr  <- lm(y~t)#
trx <- lm(y~t+x)#
dva <- lm(y~t+x2+I(1-r))#
#
abline(coef(trx)[1], coef(trx)[3], col="black")#
abline(sum(coef(trx)[1:2]), coef(trx)[3], col="black")#
#
abline(coef(dva)[1], coef(dva)[3], col="black", lty="dashed")#
abline(coef(dva)[1]+coef(dva)[2], coef(dva)[3], col="black", lty="dashed")#
trx#
dva
histogram(y0)
hist(y0)
plot(y0,r)
load("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 8/Section 8.4 Hotspots/10of30.rdata")
colnames()
ls
list=ls()
load("/Users/al/Dropbox/teaching 2012-2013/Experiments/Data and R Programs/Chapter 8/Section 8.4 Hotspots/10of30.rdata")
data <- load("/Users/al/Dropbox/teaching 2012-2013/Experiments/Data and R Programs/Chapter 8/Section 8.4 Hotspots/10of30.rdata")
data
rm(list = ls())#
#
set.seed(1337)#
#
# Establish parameters#
#
# Number of total units in the "main experimental pop"#
NS <- 30#
# Number treated#
ntreatedS <- 10#
#
# True radius for spillover#
radius <- .5#
#
# Too small radius, used for misspecification#
radiusW <- .25#
#
# Too large radius, used for generating non-experimental density "effects"#
radiusB <- .75#
#
# Number of randomizations#
numrands <- 10000#
#
# Generate coordinates#
ax <- rnorm(NS*2)#
coordsS <- cbind(ax[1:(NS)],ax[(NS+1):(2*NS)])#
#
# To multiply the population, ala Brewer, in completely separate groups. Setting this to 10 replicates the 100 out of 300 specification reported in the book.#
numrepeater <- 1#
#
N <- NS*numrepeater#
ntreated <- ntreatedS*numrepeater#
coords <- coordsS[rep(c(1:NS),numrepeater),] + cbind(2*(max(coordsS[,1])-min(coordsS[,1])+radius)*sort(rep(c(0:(numrepeater-1)),NS)),0)#
#
# Generate non-experimental units, coordinates#
NnonexpS <- 100#
coordsNonS <- cbind(rnorm(NnonexpS),rnorm(NnonexpS))#
#
Nnonexp <- NnonexpS*numrepeater#
#
coordsNon <- coordsNonS[rep(c(1:NnonexpS),numrepeater),] + cbind(2*(max(coordsS[,1])-min(coordsS[,1])+radius)*sort(rep(c(0:(numrepeater-1)),NnonexpS)),0)
Generate distance matrix#
distmat <- as.matrix(dist(coords))
distmat
non-experimental distances to experimental units#
distmatNon <- as.matrix(dist(rbind(coords,coordsNon)))[1:N,(N+1):(N+Nnonexp)]
Calculate number of units near each unit (right, small, large)#
numnear <- apply(distmat,1,function(x) sum(x < radius)) - 1#
numnearW <- apply(distmat,1,function(x) sum(x < radiusW)) - 1#
numnearB <- apply(distmat,1,function(x) sum(x < radiusB)) - 1#
#
# Calculate number of units near each nonexperiment unit (right, small, large)#
numnearNon <- apply(distmatNon,2,function(x) sum(x < radius))#
numnearNonW <- apply(distmatNon,2,function(x) sum(x < radiusW))#
numnearNonB <- apply(distmatNon,2,function(x) sum(x < radiusB))#
#
# Base POs#
Y00 <- 10 + numnearB*10
Y00
numnearB
numnearB*10
rm(list = ls())#
#
set.seed(1337)#
#
# Establish parameters#
#
# Number of total units in the "main experimental pop"#
NS <- 30#
# Number treated#
ntreatedS <- 10#
#
# True radius for spillover#
radius <- .5#
#
# Too small radius, used for misspecification#
radiusW <- .25#
#
# Too large radius, used for generating non-experimental density "effects"#
radiusB <- .75#
#
# Number of randomizations#
numrands <- 10000#
#
# Generate coordinates#
ax <- rnorm(NS*2)#
coordsS <- cbind(ax[1:(NS)],ax[(NS+1):(2*NS)])#
#
# To multiply the population, ala Brewer, in completely separate groups. Setting this to 10 replicates the 100 out of 300 specification reported in the book.#
numrepeater <- 1#
#
N <- NS*numrepeater#
ntreated <- ntreatedS*numrepeater#
coords <- coordsS[rep(c(1:NS),numrepeater),] + cbind(2*(max(coordsS[,1])-min(coordsS[,1])+radius)*sort(rep(c(0:(numrepeater-1)),NS)),0)#
#
# Generate non-experimental units, coordinates#
NnonexpS <- 100#
coordsNonS <- cbind(rnorm(NnonexpS),rnorm(NnonexpS))#
#
Nnonexp <- NnonexpS*numrepeater#
#
coordsNon <- coordsNonS[rep(c(1:NnonexpS),numrepeater),] + cbind(2*(max(coordsS[,1])-min(coordsS[,1])+radius)*sort(rep(c(0:(numrepeater-1)),NnonexpS)),0)#
########
#
# Generate distance matrix#
distmat <- as.matrix(dist(coords))#
#
# non-experimental distances to experimental units#
distmatNon <- as.matrix(dist(rbind(coords,coordsNon)))[1:N,(N+1):(N+Nnonexp)]#
#
# Calculate number of units near each unit (right, small, large)#
numnear <- apply(distmat,1,function(x) sum(x < radius)) - 1#
numnearW <- apply(distmat,1,function(x) sum(x < radiusW)) - 1#
numnearB <- apply(distmat,1,function(x) sum(x < radiusB)) - 1#
#
# Calculate number of units near each nonexperiment unit (right, small, large)#
numnearNon <- apply(distmatNon,2,function(x) sum(x < radius))#
numnearNonW <- apply(distmatNon,2,function(x) sum(x < radiusW))#
numnearNonB <- apply(distmatNon,2,function(x) sum(x < radiusB))#
#
# Base POs#
Y00 <- 10 + numnearB*10#
#
# Treatmen Effects#
t01 <- -5#
t10 <- 5#
t11 <- -7#
#
Y01 <- Y00 + t01#
Y10 <- Y00 + t10#
Y11 <- Y00 + t11#
#
# Randomly generate treatment assignments#
treat <- sample(c(rep(0,N-ntreated),rep(1,ntreated)))#
#sample(sample(sample(c(rep(1,ntreated),rep(0,N-ntreated)))))#
#
# True POs for units eligible for all treatments#
mean(Y00[numnear>0])#
mean(Y10[numnear>0])#
mean(Y01[numnear>0])#
mean(Y11[numnear>0])#
#
# Generate non-exp POs#
Y00Non <- 0 + numnearNonB#
Y10Non <- Y00Non + t10#
#
# Generate treatment conditions -- exposure condition defined by "cond"#
distmattreat <- distmat + (1-treat)*100#
numtreat <- apply(distmattreat,2,function(x) sum(x < radius)) - treat#
cond <- 10*(numtreat > 0) + treat#
#
numtreatW <- apply(distmattreat,2,function(x) sum(x < radiusW)) - treat#
condW <- 10*(numtreatW > 0) + treat#
#
distmatNontreat <- distmatNon + (1-treat)*100#
numtreatNon <- apply(distmatNontreat,2,function(x) sum(x < radius))#
condNon <- 10*(numtreatNon > 0)#
#
cond[numnear>0]#
table(numnear,treat)#
#
# Generate observed outcomes#
Y <- Y00#
Y[cond==1] <- Y01[cond==1]#
Y[cond==11] <- Y11[cond==11]#
Y[cond==10] <- Y10[cond==10]#
#
mean(Y[treat==1]) - mean(Y[treat==0])#
#
# Estimate probability of exposure for correct model and wrong-radius model#
# Order: 00, 01, 10, 11#
pmat <- pmatW <- matrix(0,N,4)
pmat
Scalar probability (10) for non-experimental units.#
pNon <- rep(0,Nnonexp)#
#
for (i in 1:numrands) {#
	treatri <- sample(treat)#
	distmattreat <- distmat + (1-treatri)*100#
	numtreat <- apply(distmattreat,2,function(x) sum(x < radius)) - treatri#
	condri <- 2*(numtreat > 0) + treatri + 1#
	for (j in 1:N) pmat[j,condri[j]] <- pmat[j,condri[j]] + 1#
	# wrong radius#
	numtreat <- apply(distmattreat,2,function(x) sum(x < radiusW)) - treatri#
	condri <- 2*(numtreat > 0) + treatri + 1#
	for (j in 1:N) pmatW[j,condri[j]] <- pmatW[j,condri[j]] + 1#
	# non-experimental units#
	distmatNontreat <- distmatNon + (1-treatri)*100#
	numtreatNon <- apply(distmatNontreat,2,function(x) sum(x < radius))#
	pNon <- pNon +  (numtreatNon > 0)#
	if (i %% 1000 == 0) cat(i,"")#
	}#
####### No one near - can estimate simple diff in means#
#
summary(lm(Y[numnear == 0]~treat[numnear==0]))#
#
####### Someone near - need to apply IPW#
#
pscoremat <- pmat/numrands#
pscorematW <- pmatW/numrands#
pscoreNon <- pNon/numrands
pscoremat
pscorematW
